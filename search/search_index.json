{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"QC Detector System","text":"<p>A Quality Control Detection System for automated measurement and inspection of products (primarily sandals/footwear). Built with Python, OpenCV for computer vision, and PySide6 for a modern desktop GUI.</p>"},{"location":"#core-capabilities","title":"Core Capabilities","text":"<ul> <li>:material-camera: Live Camera Measurement \u2014 Real-time object detection and measurement from USB or IP cameras</li> <li>:material-image: Photo Measurement \u2014 Analyze and measure objects from static images</li> <li>:material-video: Video Measurement \u2014 Process video files for batch measurements</li> <li>:material-tag: Product Profiles \u2014 Manage product SKUs with expected dimensions and tolerances</li> <li>:material-robot-industrial: PLC Integration \u2014 Modbus RTU communication for industrial automation triggers</li> <li>:material-database: Database Support \u2014 PostgreSQL backend for storing measurement results</li> </ul>"},{"location":"#detection-methods","title":"Detection Methods","text":"Method Speed Accuracy Best For Standard (Contour) \u26a1\u26a1\u26a1 Fast Good Dark objects, controlled lighting FastSAM (AI) \u26a1\u26a1 Medium Very Good Diverse object types YOLO-Seg (AI) \u26a1\u26a1 Medium Excellent \u2b50 All colors (more contrast), precision needed Advanced (YOLOv8x + SAM) \u26a1 Slower (GPU recommended) Best \u2b50\u2b50 Difficult objects (low contrast), maximum quality"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>git clone https://github.com/IT-Porto-Indonesia-Sejahtera/QC-Detector.git\ncd QC-Detector\npip install -r requirements.txt\npython main.py\n</code></pre> <p>See the Getting Started guide for detailed installation instructions.</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>QC-Detector/\n\u251c\u2500\u2500 main.py                      # Application entry point\n\u251c\u2500\u2500 app/                         # GUI application (PySide6)\n\u2502   \u251c\u2500\u2500 main_windows.py          # Main window controller\n\u2502   \u251c\u2500\u2500 pages/                   # Screen components\n\u2502   \u251c\u2500\u2500 widgets/                 # Reusable UI components\n\u2502   \u2514\u2500\u2500 utils/                   # UI utilities\n\u251c\u2500\u2500 model/                       # Computer vision &amp; measurement\n\u2502   \u251c\u2500\u2500 measurement.py           # Core measurement logic\n\u2502   \u251c\u2500\u2500 advanced_inference.py    # YOLOv8x + SAM pipeline\n\u2502   \u251c\u2500\u2500 yolo_inference.py        # YOLOv8-seg integration\n\u2502   \u251c\u2500\u2500 fastsam_inference.py     # FastSAM integration\n\u2502   \u2514\u2500\u2500 measure_live_sandals.py  # Live camera measurement\n\u251c\u2500\u2500 backend/                     # Backend services\n\u2502   \u251c\u2500\u2500 DB.py                    # PostgreSQL database\n\u2502   \u251c\u2500\u2500 aruco_utils.py           # ArUco marker detection\n\u2502   \u2514\u2500\u2500 get_product_sku.py       # Product SKU management\n\u2514\u2500\u2500 tests/                       # Unit tests\n</code></pre> <p>Version: 1.0 | Last Updated: February 2026 | Status: Production Ready</p> <p>Proprietary \u2014 PT Porto Indonesia Sejahtera</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>OS: Windows, macOS, or Linux</li> <li>Camera: USB webcam or IP camera (RTSP supported)</li> <li>Database: PostgreSQL (optional, for result storage)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/IT-Porto-Indonesia-Sejahtera/QC-Detector.git\ncd QC-Detector\n</code></pre>"},{"location":"getting-started/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"WindowsmacOSLinux <pre><code>python -m venv venv\nvenv\\Scripts\\activate\n</code></pre> <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre> <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"getting-started/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/#4-install-ai-models-optional-but-recommended","title":"4. Install AI Models (Optional but Recommended)","text":"<pre><code># For YOLOv8-seg and Advanced (YOLOv8x + SAM)\npip install ultralytics\n</code></pre> <p>Models will auto-download on first use:</p> Model Size Used By <code>yolov8n-seg.pt</code> ~13 MB YOLO-Seg detection <code>yolov8x.pt</code> ~130 MB Advanced \u2014 The Spotter <code>sam_b.pt</code> ~375 MB Advanced \u2014 The Surgeon <code>FastSAM-s.pt</code> ~24 MB FastSAM detection"},{"location":"getting-started/#5-configure-environment-variables-optional","title":"5. Configure Environment Variables (Optional)","text":"<p>Create a <code>.env</code> file in the project root for database connectivity:</p> <pre><code>DB_HOST=localhost\nDB_NAME=qc_detector\nDB_USER=app_user\nDB_PASS=your_password\nDB_PORT=5432\n</code></pre>"},{"location":"getting-started/#running-the-application","title":"Running the Application","text":"WindowsmacOSLinux <pre><code>python main.py\n# Or double-click: windows.bat\n</code></pre> <pre><code>python3 main.py\n# Or double-click: mac.command\n</code></pre> <pre><code>python3 main.py\n# Or run: ./linux.sh\n</code></pre>"},{"location":"getting-started/#application-screens","title":"Application Screens","text":"Screen Description Menu Main navigation hub Live Camera Real-time measurement with live camera feed Measure Photo Load and measure from image files Measure Video Process video files for measurements Capture Dataset Capture images for training/reference Settings Configure camera, calibration, and PLC settings Profiles Manage product profiles with expected dimensions"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#camera-not-detected","title":"Camera Not Detected","text":"<ul> <li>Ensure camera is connected before launching</li> <li>For IP cameras, verify network with <code>ping &lt;camera-ip&gt;</code></li> <li>Run <code>python find_camera.py</code> to list available cameras</li> </ul>"},{"location":"getting-started/#database-connection-failed","title":"Database Connection Failed","text":"<ul> <li>App continues without database \u2014 this is safe</li> <li>Check <code>.env</code> configuration values</li> <li>Verify PostgreSQL is running: <code>pg_ctl status</code></li> </ul>"},{"location":"getting-started/#ai-models-not-working","title":"AI Models Not Working","text":"<ul> <li>Ensure <code>ultralytics</code> is installed: <code>pip install ultralytics</code></li> <li>Models download on first use \u2014 internet connection required</li> <li>Check console output for specific error messages</li> </ul>"},{"location":"api/app/main-window/","title":"Main Window","text":"<p>The main application window controller \u2014 manages screen navigation using a <code>QStackedWidget</code>.</p>"},{"location":"api/app/main-window/#app.main_windows","title":"<code>main_windows</code>","text":""},{"location":"api/app/main-window/#app.main_windows-classes","title":"Classes","text":""},{"location":"api/app/main-window/#app.main_windows.MainWindow","title":"<code>MainWindow()</code>","text":"<p>               Bases: <code>QWidget</code></p> Source code in <code>app\\main_windows.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.setWindowTitle(\"QC Detector System\")\n    self.setMinimumSize(900, 600)\n    self.setStyleSheet(\"background-color: white;\")\n    self.from_live = False # Track if we came from live screen\n\n    self.stack = QStackedWidget()\n    layout = QVBoxLayout(self)\n    layout.addWidget(self.stack)\n\n    # Create pages\n    self.menu_page = MenuScreen(controller=self)\n    self.photo_page = MeasurePhotoScreen(controller=self)\n    self.video_page = MeasureVideoScreen(controller=self)\n    self.live_page = LiveCameraScreen(parent=self)\n    self.dataset_page = CaptureDatasetScreen(parent=self)\n    self.settings_page = GeneralSettingsPage(controller=self)\n    self.profiles_page = ProfilesPage(controller=self)\n\n    # Add to stack\n    self.stack.addWidget(self.menu_page)\n    self.stack.addWidget(self.photo_page)\n    self.stack.addWidget(self.video_page)\n    self.stack.addWidget(self.live_page)\n    self.stack.addWidget(self.dataset_page)\n    self.stack.addWidget(self.settings_page)\n    self.stack.addWidget(self.profiles_page)\n\n    self.stack.setCurrentWidget(self.menu_page)\n\n    # --- AI Model Warmup ---\n    try:\n        from model.inference_utils import ModelWarmupWorker\n        self.warmup_worker = ModelWarmupWorker(self)\n        self.warmup_worker.start()\n        print(\"[Startup] Background AI model warmup started.\")\n    except Exception as e:\n        print(f\"[Startup] Error starting warmup: {e}\")\n\n\n    # --- Internal Scheduler Setup ---\n    from PySide6.QtCore import QTimer, QTime\n    from backend.get_product_sku import ProductSKUWorker\n\n    self.ProductSKUWorker = ProductSKUWorker\n    self.scheduler_timer = QTimer(self)\n    self.scheduler_timer.timeout.connect(self.check_scheduler)\n    self.scheduler_timer.start(60000) # Check every 60 seconds\n\n    # Load scheduler settings\n    self.refresh_scheduler_settings()\n\n    log_info(f\"[Scheduler] Internal scheduler started. Mode: {self.scheduler_mode}\")\n</code></pre>"},{"location":"api/app/main-window/#app.main_windows.MainWindow-functions","title":"Functions","text":""},{"location":"api/app/main-window/#app.main_windows.MainWindow.refresh_scheduler_settings","title":"<code>refresh_scheduler_settings()</code>","text":"<p>Load scheduler configuration from app_settings.json</p> Source code in <code>app\\main_windows.py</code> <pre><code>def refresh_scheduler_settings(self):\n    \"\"\"Load scheduler configuration from app_settings.json\"\"\"\n    settings_file = os.path.join(\"output\", \"settings\", \"app_settings.json\")\n    settings = JsonUtility.load_from_json(settings_file) or {}\n\n    # Scheduling Modes: \"daily\" (legacy), \"interval\" (minutes), \"schedule\" (specific times)\n    self.scheduler_mode = settings.get(\"scheduler_mode\", \"daily\")\n    self.scheduler_interval_min = settings.get(\"scheduler_interval_min\", 60)\n    self.scheduler_schedule_times = settings.get(\"scheduler_schedule_times\", [\"09:00\"])\n\n    # Legacy single-time support\n    self.scheduled_hour = settings.get(\"scheduler_hour\", 9) \n    self.scheduled_minute = settings.get(\"scheduler_minute\", 0)\n\n    # State tracking\n    self.last_run_time = None \n    self.last_run_date = None\n</code></pre>"},{"location":"api/app/main-window/#app.main_windows-functions","title":"Functions","text":""},{"location":"api/app/main-window/#app.main_windows.run_app","title":"<code>run_app()</code>","text":"Source code in <code>app\\main_windows.py</code> <pre><code>def run_app():\n    # Enable High DPI scaling and set rounding policy for fractional scaling\n    from PySide6.QtCore import Qt\n    from PySide6.QtGui import QGuiApplication\n    QGuiApplication.setHighDpiScaleFactorRoundingPolicy(Qt.HighDpiScaleFactorRoundingPolicy.PassThrough)\n\n    app = QApplication(sys.argv)\n    window = MainWindow()\n    window.showMaximized()\n    sys.exit(app.exec())\n</code></pre>"},{"location":"api/app/pages/","title":"Pages","text":"<p>Application screens \u2014 each page is a <code>QWidget</code> that handles its own UI and logic.</p>"},{"location":"api/app/pages/#menu-screen","title":"Menu Screen","text":""},{"location":"api/app/pages/#app.pages.menu_screen","title":"<code>menu_screen</code>","text":""},{"location":"api/app/pages/#app.pages.menu_screen-classes","title":"Classes","text":""},{"location":"api/app/pages/#live-camera-screen","title":"Live Camera Screen","text":""},{"location":"api/app/pages/#app.pages.measure_live_screen","title":"<code>measure_live_screen</code>","text":""},{"location":"api/app/pages/#app.pages.measure_live_screen-classes","title":"Classes","text":""},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen","title":"<code>LiveCameraScreen(parent=None)</code>","text":"<p>               Bases: <code>QWidget</code></p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen-functions","title":"Functions","text":""},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.refresh_data","title":"<code>refresh_data()</code>","text":"<p>Refresh settings and profile data from JSON files.</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.setup_minimal_layout","title":"<code>setup_minimal_layout()</code>","text":"<p>Minimal Layout: Full-screen preview with overlay controls.</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.create_settings_menu","title":"<code>create_settings_menu()</code>","text":"<p>Placeholder for settings menu (now managed via General Settings page).</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.on_frame_received","title":"<code>on_frame_received(frame)</code>","text":"<p>Called by VideoCaptureThread when a new frame is available</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.setup_sensor","title":"<code>setup_sensor()</code>","text":"<p>Initialize sensor trigger</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.on_sensor_trigger","title":"<code>on_sensor_trigger()</code>","text":"<p>Called when sensor detects object within threshold</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.on_sensor_connection_change","title":"<code>on_sensor_connection_change(connected, message)</code>","text":"<p>Called when sensor connection status changes</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.start_sensor","title":"<code>start_sensor()</code>","text":"<p>Start sensor reading in background</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.stop_sensor","title":"<code>stop_sensor()</code>","text":"<p>Stop sensor reading</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.setup_plc_trigger","title":"<code>setup_plc_trigger()</code>","text":"<p>Initialize PLC Modbus trigger</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.on_plc_trigger","title":"<code>on_plc_trigger()</code>","text":"<p>Called when PLC register changes from 0 to 1</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.on_plc_value_update","title":"<code>on_plc_value_update(value)</code>","text":"<p>Called when PLC register value is read</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.on_plc_connection_change","title":"<code>on_plc_connection_change(connected, message)</code>","text":"<p>Called when PLC connection status changes</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.start_plc_trigger","title":"<code>start_plc_trigger()</code>","text":"<p>Start PLC Modbus polling in background</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen.LiveCameraScreen.stop_plc_trigger","title":"<code>stop_plc_trigger()</code>","text":"<p>Stop PLC Modbus polling</p>"},{"location":"api/app/pages/#app.pages.measure_live_screen-functions","title":"Functions","text":""},{"location":"api/app/pages/#measure-photo-screen","title":"Measure Photo Screen","text":""},{"location":"api/app/pages/#app.pages.measure_photo_screen","title":"<code>measure_photo_screen</code>","text":""},{"location":"api/app/pages/#app.pages.measure_photo_screen-classes","title":"Classes","text":""},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen","title":"<code>MeasurePhotoScreen(controller)</code>","text":"<p>               Bases: <code>QWidget</code></p>"},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen-functions","title":"Functions","text":""},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen.load_thumbnails","title":"<code>load_thumbnails()</code>","text":"<p>Load all images in input folder as thumbnails.</p>"},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen.select_image","title":"<code>select_image(path)</code>","text":"<p>Handle image selection.</p>"},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen.showEvent","title":"<code>showEvent(event)</code>","text":"<p>Called when the screen becomes visible - reload thumbnails.</p>"},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen.display_pixmap_scaled","title":"<code>display_pixmap_scaled(pixmap)</code>","text":"<p>Scale image to fit label while preserving aspect ratio (with padding).</p>"},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen.resizeEvent","title":"<code>resizeEvent(event)</code>","text":"<p>When window resizes, rescale current image properly.</p>"},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen.cv2_to_pixmap","title":"<code>cv2_to_pixmap(cv_img)</code>","text":"<p>Convert an OpenCV image (BGR) to QPixmap.</p>"},{"location":"api/app/pages/#app.pages.measure_photo_screen.MeasurePhotoScreen.measure_image","title":"<code>measure_image()</code>","text":"<p>Run measurement and show result in GUI.</p>"},{"location":"api/app/pages/#measure-video-screen","title":"Measure Video Screen","text":""},{"location":"api/app/pages/#app.pages.measure_video_screen","title":"<code>measure_video_screen</code>","text":""},{"location":"api/app/pages/#app.pages.measure_video_screen-classes","title":"Classes","text":""},{"location":"api/app/pages/#capture-dataset-screen","title":"Capture Dataset Screen","text":""},{"location":"api/app/pages/#app.pages.capture_dataset_screen","title":"<code>capture_dataset_screen</code>","text":""},{"location":"api/app/pages/#app.pages.capture_dataset_screen-classes","title":"Classes","text":""},{"location":"api/app/pages/#app.pages.capture_dataset_screen.CaptureDatasetScreen","title":"<code>CaptureDatasetScreen(parent=None)</code>","text":"<p>               Bases: <code>QWidget</code></p>"},{"location":"api/app/pages/#app.pages.capture_dataset_screen.CaptureDatasetScreen-functions","title":"Functions","text":""},{"location":"api/app/pages/#app.pages.capture_dataset_screen.CaptureDatasetScreen.detect_cameras","title":"<code>detect_cameras(max_test=3)</code>","text":"<p>Detect available cameras</p>"},{"location":"api/app/pages/#app.pages.capture_dataset_screen.CaptureDatasetScreen.showEvent","title":"<code>showEvent(event)</code>","text":"<p>Refresh camera list and start.</p>"},{"location":"api/app/pages/#app.pages.capture_dataset_screen-functions","title":"Functions","text":""},{"location":"api/app/pages/#settings-page","title":"Settings Page","text":""},{"location":"api/app/pages/#app.pages.general_settings_page","title":"<code>general_settings_page</code>","text":""},{"location":"api/app/pages/#app.pages.general_settings_page-classes","title":"Classes","text":""},{"location":"api/app/pages/#app.pages.general_settings_page.GeneralSettingsPage","title":"<code>GeneralSettingsPage(controller=None)</code>","text":"<p>               Bases: <code>QWidget</code></p>"},{"location":"api/app/pages/#app.pages.general_settings_page.GeneralSettingsPage-functions","title":"Functions","text":""},{"location":"api/app/pages/#app.pages.general_settings_page.GeneralSettingsPage.update_live_params","title":"<code>update_live_params()</code>","text":"<p>Update running capture thread with current UI values</p>"},{"location":"api/app/pages/#app.pages.general_settings_page.GeneralSettingsPage.apply_quick_settings","title":"<code>apply_quick_settings()</code>","text":"<p>Save settings without leaving the page</p>"},{"location":"api/app/pages/#app.pages.general_settings_page.GeneralSettingsPage.save_settings","title":"<code>save_settings()</code>","text":"<p>Save settings and return to live feed</p>"},{"location":"api/app/pages/#app.pages.general_settings_page.GeneralSettingsPage.run_auto_calibration","title":"<code>run_auto_calibration()</code>","text":"<p>Run ArUco-based auto calibration for mm/px</p>"},{"location":"api/app/pages/#app.pages.general_settings_page.GeneralSettingsPage.parse_aspect_ratio_input","title":"<code>parse_aspect_ratio_input(text)</code>","text":"<p>Parse text input like '16:9', '16/9', '1.77' into a float</p>"},{"location":"api/app/pages/#app.pages.general_settings_page.GeneralSettingsPage.fetch_sku_data","title":"<code>fetch_sku_data()</code>","text":"<p>Fetch product SKU data from database asynchronously.</p>"},{"location":"api/app/pages/#profiles-page","title":"Profiles Page","text":""},{"location":"api/app/pages/#app.pages.profiles_page","title":"<code>profiles_page</code>","text":""},{"location":"api/app/pages/#app.pages.profiles_page-classes","title":"Classes","text":""},{"location":"api/app/pages/#app.pages.profiles_page.ProfilesPage","title":"<code>ProfilesPage(controller=None)</code>","text":"<p>               Bases: <code>QWidget</code></p>"},{"location":"api/app/pages/#app.pages.profiles_page.ProfilesPage-functions","title":"Functions","text":""},{"location":"api/app/pages/#app.pages.profiles_page.ProfilesPage.fetch_sku_data","title":"<code>fetch_sku_data()</code>","text":"<p>Fetch product SKU data from database asynchronously.</p>"},{"location":"api/app/utilities/","title":"Utilities","text":"<p>Shared utility modules used across the application.</p>"},{"location":"api/app/utilities/#camera-utilities","title":"Camera Utilities","text":""},{"location":"api/app/utilities/#app.utils.camera_utils","title":"<code>camera_utils</code>","text":""},{"location":"api/app/utilities/#app.utils.camera_utils-functions","title":"Functions","text":""},{"location":"api/app/utilities/#app.utils.camera_utils.open_video_capture","title":"<code>open_video_capture(source, buffer_size=1, timeout_ms=3000, force_width=0, force_height=0)</code>","text":"<p>Unified function to open a cv2.VideoCapture with proper settings for RTSP, HTTP, and USB cameras.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>Camera index (int), RTSP/HTTP URL (str), or a preset dict.</p> required <code>buffer_size</code> <p>Buffer size for the capture. Default is 1 for low latency.</p> <code>1</code> <code>timeout_ms</code> <p>Timeout in milliseconds for opening and reading.</p> <code>3000</code> <code>force_width</code> <p>Forced width (0 for auto).</p> <code>0</code> <code>force_height</code> <p>Forced height (0 for auto).</p> <code>0</code> <p>Returns:</p> Type Description <p>cv2.VideoCapture: The opened capture object.</p>"},{"location":"api/app/utilities/#capture-thread","title":"Capture Thread","text":""},{"location":"api/app/utilities/#app.utils.capture_thread","title":"<code>capture_thread</code>","text":""},{"location":"api/app/utilities/#app.utils.capture_thread-classes","title":"Classes","text":""},{"location":"api/app/utilities/#app.utils.capture_thread.VideoCaptureThread","title":"<code>VideoCaptureThread(source, is_ip=False, crop_params=None, distortion_params=None, aspect_ratio_correction=1.0, force_width=0, force_height=0)</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Background thread for camera connection and frame capture</p>"},{"location":"api/app/utilities/#app.utils.capture_thread.VideoCaptureThread-functions","title":"Functions","text":""},{"location":"api/app/utilities/#app.utils.capture_thread.VideoCaptureThread.apply_distortion_correction","title":"<code>apply_distortion_correction(frame)</code>","text":"<p>Apply Lens Distortion Correction (undistort)</p>"},{"location":"api/app/utilities/#app.utils.capture_thread.VideoCaptureThread.apply_aspect_ratio_correction","title":"<code>apply_aspect_ratio_correction(frame)</code>","text":"<p>Apply manual aspect ratio correction by resizing width</p>"},{"location":"api/app/utilities/#app.utils.capture_thread.VideoCaptureThread.apply_crop","title":"<code>apply_crop(frame)</code>","text":"<p>Apply percentage-based cropping to the frame</p>"},{"location":"api/app/utilities/#app.utils.capture_thread.VideoCaptureThread.apply_rotation","title":"<code>apply_rotation(frame)</code>","text":"<p>Rotate the frame by 90, 180, or 270 degrees CW</p>"},{"location":"api/app/utilities/#app.utils.capture_thread.VideoCaptureThread.update_params","title":"<code>update_params(crop_params=None, distortion_params=None, aspect_ratio_correction=None)</code>","text":"<p>Update crop and distortion parameters dynamically</p>"},{"location":"api/app/utilities/#app.utils.capture_thread-functions","title":"Functions","text":""},{"location":"api/app/utilities/#consistency-test-thread","title":"Consistency Test Thread","text":""},{"location":"api/app/utilities/#app.utils.consistency_test_thread","title":"<code>consistency_test_thread</code>","text":""},{"location":"api/app/utilities/#app.utils.consistency_test_thread-classes","title":"Classes","text":""},{"location":"api/app/utilities/#app.utils.consistency_test_thread.ConsistencyTestThread","title":"<code>ConsistencyTestThread(video_thread, num_attempts=100, model_type='yolo', output_dir='output', mm_per_px=0.21, capture_images=False)</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Runs consistency test by capturing N frames from the video thread and running measurement inference on them. Optionally saves images and exports to Excel.</p>"},{"location":"api/app/utilities/#image-loader","title":"Image Loader","text":""},{"location":"api/app/utilities/#app.utils.image_loader","title":"<code>image_loader</code>","text":""},{"location":"api/app/utilities/#app.utils.image_loader-classes","title":"Classes","text":""},{"location":"api/app/utilities/#app.utils.image_loader.ImageLoaderSignals","title":"<code>ImageLoaderSignals</code>","text":"<p>               Bases: <code>QObject</code></p> <p>Signals for the ImageLoaderWorker. Must be a QObject to support signals.</p>"},{"location":"api/app/utilities/#app.utils.image_loader.ImageLoaderWorker","title":"<code>ImageLoaderWorker(gdrive_id, cancelled_flag)</code>","text":"<p>               Bases: <code>QRunnable</code></p> <p>Worker thread to download an image from a URL.</p>"},{"location":"api/app/utilities/#app.utils.image_loader.NetworkImageLoader","title":"<code>NetworkImageLoader(parent=None)</code>","text":"<p>               Bases: <code>QObject</code></p> <p>Manager for loading network images with caching.</p>"},{"location":"api/app/utilities/#app.utils.image_loader.NetworkImageLoader-functions","title":"Functions","text":""},{"location":"api/app/utilities/#app.utils.image_loader.NetworkImageLoader.load_image","title":"<code>load_image(gdrive_id)</code>","text":"<p>Request to load an image. If cached, emits image_loaded immediately. If downloading, waits. If new, starts download.</p>"},{"location":"api/app/utilities/#app.utils.image_loader.NetworkImageLoader.cancel_all","title":"<code>cancel_all()</code>","text":"<p>Cancel all pending downloads. Call before destroying the loader.</p>"},{"location":"api/app/utilities/#theme-manager","title":"Theme Manager","text":""},{"location":"api/app/utilities/#app.utils.theme_manager","title":"<code>theme_manager</code>","text":""},{"location":"api/app/utilities/#app.utils.theme_manager-classes","title":"Classes","text":""},{"location":"api/app/utilities/#app.utils.theme_manager.ThemeManager","title":"<code>ThemeManager</code>","text":""},{"location":"api/app/utilities/#app.utils.theme_manager.ThemeManager-functions","title":"Functions","text":""},{"location":"api/app/utilities/#app.utils.theme_manager.ThemeManager.is_dark_mode","title":"<code>is_dark_mode()</code>  <code>staticmethod</code>","text":"<p>Returns True if current time is between 6 PM and 6 AM.</p>"},{"location":"api/app/utilities/#app.utils.theme_manager.ThemeManager.apply_theme_to_widget","title":"<code>apply_theme_to_widget(widget, theme=None)</code>  <code>staticmethod</code>","text":"<p>Helper to set generic window style.</p>"},{"location":"api/app/utilities/#ui-scaling","title":"UI Scaling","text":""},{"location":"api/app/utilities/#app.utils.ui_scaling","title":"<code>ui_scaling</code>","text":""},{"location":"api/app/utilities/#app.utils.ui_scaling-classes","title":"Classes","text":""},{"location":"api/app/utilities/#app.utils.ui_scaling.UIScaling","title":"<code>UIScaling</code>","text":"<p>Utility for scaling UI elements based on screen resolution.</p>"},{"location":"api/app/utilities/#app.utils.ui_scaling.UIScaling-functions","title":"Functions","text":""},{"location":"api/app/utilities/#app.utils.ui_scaling.UIScaling.get_scale_factor","title":"<code>get_scale_factor()</code>  <code>classmethod</code>","text":"<p>Calculate the scale factor based on the current primary screen resolution.</p>"},{"location":"api/app/utilities/#app.utils.ui_scaling.UIScaling.scale","title":"<code>scale(px)</code>  <code>classmethod</code>","text":"<p>Scale a pixel value.</p>"},{"location":"api/app/utilities/#app.utils.ui_scaling.UIScaling.scale_font","title":"<code>scale_font(size)</code>  <code>classmethod</code>","text":"<p>Scale a font size.</p>"},{"location":"api/app/utilities/#app.utils.ui_scaling.UIScaling.get_screen_size","title":"<code>get_screen_size()</code>  <code>classmethod</code>","text":"<p>Get the current primary screen size.</p>"},{"location":"api/backend/aruco-utils/","title":"ArUco Utils","text":"<p>ArUco marker detection and multi-marker calibration utilities.</p> <p>Supports single-marker and inter-marker distance calibration using a known A3 calibration sheet layout.</p>"},{"location":"api/backend/aruco-utils/#backend.aruco_utils","title":"<code>aruco_utils</code>","text":""},{"location":"api/backend/aruco-utils/#backend.aruco_utils-functions","title":"Functions","text":""},{"location":"api/backend/aruco-utils/#backend.aruco_utils.get_physical_distance","title":"<code>get_physical_distance(id1, id2)</code>","text":"<p>Returns the physical distance (mm) between two marker centers  based on the known A3 layout.</p> Source code in <code>backend\\aruco_utils.py</code> <pre><code>def get_physical_distance(id1, id2):\n    \"\"\"\n    Returns the physical distance (mm) between two marker centers \n    based on the known A3 layout.\n    \"\"\"\n    if id1 in A3_MARKER_LAYOUT and id2 in A3_MARKER_LAYOUT:\n        p1 = np.array(A3_MARKER_LAYOUT[id1])\n        p2 = np.array(A3_MARKER_LAYOUT[id2])\n        return float(np.linalg.norm(p1 - p2))\n    return None\n</code></pre>"},{"location":"api/backend/aruco-utils/#backend.aruco_utils.detect_aruco_markers","title":"<code>detect_aruco_markers(frame, marker_size_mm, dictionary_id=cv2.aruco.DICT_4X4_250)</code>","text":"<p>Detects multiple ArUco markers and calculates mm/px using: 1. Individual marker sizes (basic method) 2. Inter-marker distances (high-precision method when 2+ markers detected)</p> <p>For inter-marker distance calculation, we use known marker center positions from the calibration sheet layout.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(success, result_dict)</p> Source code in <code>backend\\aruco_utils.py</code> <pre><code>def detect_aruco_markers(frame, marker_size_mm, dictionary_id=cv2.aruco.DICT_4X4_250):\n    \"\"\"\n    Detects multiple ArUco markers and calculates mm/px using:\n    1. Individual marker sizes (basic method)\n    2. Inter-marker distances (high-precision method when 2+ markers detected)\n\n    For inter-marker distance calculation, we use known marker center positions\n    from the calibration sheet layout.\n\n    Returns:\n        tuple: (success, result_dict)\n    \"\"\"\n    if frame is None:\n        return False, {\"error\": \"Invalid frame\", \"code\": \"INVALID_FRAME\"}\n\n    # Initialize the aruco detector\n    aruco_dict = cv2.aruco.getPredefinedDictionary(dictionary_id)\n    aruco_params = cv2.aruco.DetectorParameters()\n\n    # Robust detection parameters\n    aruco_params.adaptiveThreshWinSizeMin = 3\n    aruco_params.adaptiveThreshWinSizeMax = 53\n    aruco_params.adaptiveThreshWinSizeStep = 4\n    aruco_params.minMarkerPerimeterRate = 0.01\n    aruco_params.maxMarkerPerimeterRate = 4.0\n    aruco_params.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_SUBPIX\n\n    detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params)\n\n    # Convert to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Image quality metrics\n    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n    mean_brightness = np.mean(gray)\n\n    # Detect markers\n    corners, ids, rejected = detector.detectMarkers(gray)\n\n    # Refine corners for higher accuracy\n    if ids is not None and len(ids) &gt; 0:\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.001)\n        for corner in corners:\n            cv2.cornerSubPix(gray, corner, (5, 5), (-1, -1), criteria)\n\n    if ids is None or len(ids) == 0:\n        msg = \"No ArUco marker detected.\"\n        if laplacian_var &lt; 100:\n            msg += \" Image seems blurry.\"\n        if mean_brightness &lt; 40:\n            msg += \" Too dark.\"\n        elif mean_brightness &gt; 220:\n            msg += \" Overexposed.\"\n\n        return False, {\n            \"error\": msg, \n            \"code\": \"NO_MARKER\",\n            \"sharpness\": laplacian_var,\n            \"brightness\": mean_brightness,\n            \"rejected_count\": len(rejected) if rejected else 0\n        }\n\n    # Process all detected markers\n    marker_data = []\n    mmpx_from_sizes = []\n\n    for i, marker_id in enumerate(ids):\n        marker_corners = corners[i][0]\n\n        # Calculate pixel size of this marker\n        side1 = np.linalg.norm(marker_corners[0] - marker_corners[1])\n        side2 = np.linalg.norm(marker_corners[1] - marker_corners[2])\n        side3 = np.linalg.norm(marker_corners[2] - marker_corners[3])\n        side4 = np.linalg.norm(marker_corners[3] - marker_corners[0])\n\n        avg_side_px = (side1 + side2 + side3 + side4) / 4.0\n        mmpx = marker_size_mm / avg_side_px\n        mmpx_from_sizes.append(mmpx)\n\n        # Calculate center\n        center = np.mean(marker_corners, axis=0)\n\n        # Check for tilt\n        ratio_h = max(side1, side3) / min(side1, side3) if min(side1, side3) &gt; 0 else 999\n        ratio_v = max(side2, side4) / min(side2, side4) if min(side2, side4) &gt; 0 else 999\n        is_tilted = ratio_h &gt; 1.15 or ratio_v &gt; 1.15\n\n        marker_data.append({\n            \"id\": int(marker_id[0]),\n            \"center\": center,\n            \"corners\": marker_corners,\n            \"pixel_size\": avg_side_px,\n            \"mmpx\": mmpx,\n            \"is_tilted\": is_tilted\n        })\n\n    # Calculate mm/px from marker sizes (basic method)\n    avg_mmpx_size = float(np.mean(mmpx_from_sizes))\n    std_mmpx_size = float(np.std(mmpx_from_sizes))\n\n    # ========== HIGH-PRECISION: Inter-marker distance calculation ==========\n    # If we have 2+ markers, we can calculate mm/px from their relative positions\n    # This is MORE ACCURATE than using marker size alone because:\n    # 1. Distances are larger -&gt; less pixel quantization error\n    # 2. Any systematic marker size error is eliminated\n\n    mmpx_from_distances = []\n    distance_pairs = []\n\n    if len(marker_data) &gt;= 2:\n        # For each pair of markers, calculate the distance\n        for m1, m2 in combinations(marker_data, 2):\n            # Pixel distance between marker centers\n            px_distance = np.linalg.norm(m1[\"center\"] - m2[\"center\"])\n\n            # Check if we know the physical distance between these IDs\n            physical_dist = get_physical_distance(m1[\"id\"], m2[\"id\"])\n\n            if px_distance &gt; 50 and physical_dist is not None:\n                mmpx_dist = physical_dist / px_distance\n                mmpx_from_distances.append(mmpx_dist)\n\n                distance_pairs.append({\n                    \"id1\": m1[\"id\"],\n                    \"id2\": m2[\"id\"],\n                    \"px_distance\": float(px_distance),\n                    \"mm_distance\": float(physical_dist),\n                    \"mmpx\": float(mmpx_dist)\n                })\n            elif px_distance &gt; 50:\n                # Still record the pair even if we don't have layout knowledge\n                distance_pairs.append({\n                    \"id1\": m1[\"id\"],\n                    \"id2\": m2[\"id\"],\n                    \"px_distance\": float(px_distance),\n                    \"mm_distance\": None,\n                    \"mmpx\": None\n                })\n\n    # ========== FINAL mm/px CALCULATION ==========\n    # Prefer inter-marker distance method if available (more accurate)\n    if mmpx_from_distances:\n        final_mmpx = float(np.mean(mmpx_from_distances))\n        method_used = \"inter_marker_distance\"\n        # Accuracy of measurement relative to marker size method\n        std_mmpx = float(np.std(mmpx_from_distances))\n    else:\n        final_mmpx = avg_mmpx_size\n        method_used = \"marker_size\"\n        std_mmpx = std_mmpx_size\n\n    # Calculate stability score (how consistent are the measurements)\n    # 100% means zero variance, lower means higher variance\n    if len(mmpx_from_sizes) &gt; 1:\n        # We still use mmpx_from_sizes for general sanity check of marker consistency\n        stability = 100 - min(100, (std_mmpx_size / avg_mmpx_size) * 1000) if avg_mmpx_size &gt; 0 else 0\n    else:\n        stability = 100.0  # Single marker = no variance to measure\n\n    # Check if any marker is tilted\n    any_tilted = any(m[\"is_tilted\"] for m in marker_data)\n\n    # Draw results\n    out_frame = frame.copy()\n    cv2.aruco.drawDetectedMarkers(out_frame, corners, ids)\n\n    # Draw info for each marker\n    for m in marker_data:\n        center = m[\"center\"].astype(int)\n        color = (0, 255, 0) if not m[\"is_tilted\"] else (0, 165, 255)\n        # Use short text to keep it clean\n        cv2.putText(out_frame, f\"ID{m['id']}\", \n                    (center[0] - 20, center[1] - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n\n    # Draw distance lines between markers (for debugging)\n    for pair in distance_pairs:\n        if pair.get(\"mmpx\") is not None:\n            m1 = next(m for m in marker_data if m[\"id\"] == pair[\"id1\"])\n            m2 = next(m for m in marker_data if m[\"id\"] == pair[\"id2\"])\n            pt1 = tuple(m1[\"center\"].astype(int))\n            pt2 = tuple(m2[\"center\"].astype(int))\n            cv2.line(out_frame, pt1, pt2, (255, 0, 255), 1)\n\n    # Draw summary text\n    cv2.putText(out_frame, f\"mm/px: {final_mmpx:.6f} ({method_used})\", \n                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n    cv2.putText(out_frame, f\"Markers: {len(ids)} | Stability: {stability:.1f}%\", \n                (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    if any_tilted:\n        cv2.putText(out_frame, \"WARNING: Tilt detected\", \n                    (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n\n    return True, {\n        \"success\": True,\n        \"mm_per_px\": final_mmpx,\n        \"marker_count\": len(ids),\n        \"markers\": marker_data,\n        \"distance_pairs\": distance_pairs,\n        \"method\": method_used,\n        \"std_dev\": std_mmpx,\n        \"stability\": stability,\n        \"is_tilted\": any_tilted,\n        \"annotated_frame\": out_frame,\n        \"sharpness\": laplacian_var,\n        \"brightness\": mean_brightness\n    }\n</code></pre>"},{"location":"api/backend/aruco-utils/#backend.aruco_utils.detect_aruco_marker","title":"<code>detect_aruco_marker(frame, marker_size_mm, dictionary_id=cv2.aruco.DICT_4X4_250)</code>","text":"<p>Alias for backward compatibility - calls the multi-marker version.</p> Source code in <code>backend\\aruco_utils.py</code> <pre><code>def detect_aruco_marker(frame, marker_size_mm, dictionary_id=cv2.aruco.DICT_4X4_250):\n    \"\"\"Alias for backward compatibility - calls the multi-marker version.\"\"\"\n    return detect_aruco_markers(frame, marker_size_mm, dictionary_id)\n</code></pre>"},{"location":"api/backend/database/","title":"Database (DB)","text":"<p>PostgreSQL database module with connection pooling and query functions.</p>"},{"location":"api/backend/database/#backend.DB","title":"<code>DB</code>","text":"<p>Database module for QC-Detector backend. Provides PostgreSQL database connection pool and query functions.</p>"},{"location":"api/backend/database/#backend.DB-classes","title":"Classes","text":""},{"location":"api/backend/database/#backend.DB.DatabaseError","title":"<code>DatabaseError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for database errors.</p>"},{"location":"api/backend/database/#backend.DB.DatabaseConnectionError","title":"<code>DatabaseConnectionError</code>","text":"<p>               Bases: <code>DatabaseError</code></p> <p>Exception raised when database connection fails.</p>"},{"location":"api/backend/database/#backend.DB.DatabaseQueryError","title":"<code>DatabaseQueryError</code>","text":"<p>               Bases: <code>DatabaseError</code></p> <p>Exception raised when a query fails.</p>"},{"location":"api/backend/database/#backend.DB-functions","title":"Functions","text":""},{"location":"api/backend/database/#backend.DB.init_db","title":"<code>init_db()</code>","text":"<p>Initialize the database connection pool. Reads configuration from environment variables.</p> Environment variables <p>DB_HOST: Database host (default: localhost) DB_NAME: Database name (default: mydb) DB_USER: Database user (default: app_user) DB_PASS: Database password (default: secret) DB_PORT: Database port (default: 5432) DB_MIN_CONN: Minimum connections in pool (default: 1) DB_MAX_CONN: Maximum connections in pool (default: 5)</p> <p>Returns:</p> Name Type Description <code>SimpleConnectionPool</code> <code>SimpleConnectionPool</code> <p>The initialized connection pool</p> <p>Raises:</p> Type Description <code>DatabaseConnectionError</code> <p>If connection to database fails</p> Source code in <code>backend\\DB.py</code> <pre><code>def init_db() -&gt; SimpleConnectionPool:\n    \"\"\"\n    Initialize the database connection pool.\n    Reads configuration from environment variables.\n\n    Environment variables:\n        DB_HOST: Database host (default: localhost)\n        DB_NAME: Database name (default: mydb)\n        DB_USER: Database user (default: app_user)\n        DB_PASS: Database password (default: secret)\n        DB_PORT: Database port (default: 5432)\n        DB_MIN_CONN: Minimum connections in pool (default: 1)\n        DB_MAX_CONN: Maximum connections in pool (default: 5)\n\n    Returns:\n        SimpleConnectionPool: The initialized connection pool\n\n    Raises:\n        DatabaseConnectionError: If connection to database fails\n    \"\"\"\n    global _db_pool\n\n    if _db_pool is not None:\n        return _db_pool\n\n    try:\n        _db_pool = SimpleConnectionPool(\n            minconn=int(os.getenv(\"DB_MIN_CONN\", \"1\")),\n            maxconn=int(os.getenv(\"DB_MAX_CONN\", \"5\")),\n            host=os.getenv(\"DB_HOST\", \"localhost\"),\n            database=os.getenv(\"DB_NAME\", \"mydb\"),\n            user=os.getenv(\"DB_USER\", \"app_user\"),\n            password=os.getenv(\"DB_PASS\") or os.getenv(\"DB_PASSWORD\", \"secret\"),\n            port=int(os.getenv(\"DB_PORT\", \"5432\")),\n            connect_timeout=10,  # 10 second connection timeout\n            # TCP keepalive: server detects dead connections in ~30 seconds\n            keepalives=1,              # Enable TCP keepalive\n            keepalives_idle=10,        # Seconds before sending keepalive probe\n            keepalives_interval=5,     # Seconds between keepalive probes\n            keepalives_count=3         # Number of failed probes before closing\n        )\n        logger.info(\"Database connection pool initialized successfully\")\n        return _db_pool\n    except OperationalError as e:\n        logger.error(f\"Failed to connect to database: {e}\")\n        raise DatabaseConnectionError(f\"Failed to connect to database: {e}\") from e\n    except ValueError as e:\n        logger.error(f\"Invalid database configuration: {e}\")\n        raise DatabaseConnectionError(f\"Invalid database configuration: {e}\") from e\n</code></pre>"},{"location":"api/backend/database/#backend.DB.get_pool","title":"<code>get_pool()</code>","text":"<p>Get the database connection pool, initializing if necessary.</p> <p>Returns:</p> Name Type Description <code>SimpleConnectionPool</code> <code>Optional[SimpleConnectionPool]</code> <p>The connection pool, or None if not initialized</p> Source code in <code>backend\\DB.py</code> <pre><code>def get_pool() -&gt; Optional[SimpleConnectionPool]:\n    \"\"\"\n    Get the database connection pool, initializing if necessary.\n\n    Returns:\n        SimpleConnectionPool: The connection pool, or None if not initialized\n    \"\"\"\n    global _db_pool\n    if _db_pool is None:\n        try:\n            init_db()\n        except DatabaseConnectionError:\n            return None\n    return _db_pool\n</code></pre>"},{"location":"api/backend/database/#backend.DB.is_connected","title":"<code>is_connected()</code>","text":"<p>Check if the database connection pool is initialized and available.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if connected, False otherwise</p> Source code in <code>backend\\DB.py</code> <pre><code>def is_connected() -&gt; bool:\n    \"\"\"\n    Check if the database connection pool is initialized and available.\n\n    Returns:\n        bool: True if connected, False otherwise\n    \"\"\"\n    return _db_pool is not None\n</code></pre>"},{"location":"api/backend/database/#backend.DB.fetch_all","title":"<code>fetch_all(query, params=None, as_dict=False)</code>","text":"<p>Execute a SELECT query and fetch all results.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string (can use %s or %(name)s placeholders)</p> required <code>params</code> <code>Optional[Union[Tuple, Dict]]</code> <p>Query parameters as tuple or dict (optional)</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return results as list of dicts instead of tuples</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of result rows (as tuples or dicts based on as_dict parameter)</p> <code>List[Any]</code> <p>Returns empty list if database is not connected or query fails</p> Example Source code in <code>backend\\DB.py</code> <pre><code>def fetch_all(\n    query: str,\n    params: Optional[Union[Tuple, Dict]] = None,\n    as_dict: bool = False\n) -&gt; List[Any]:\n    \"\"\"\n    Execute a SELECT query and fetch all results.\n\n    Args:\n        query: SQL query string (can use %s or %(name)s placeholders)\n        params: Query parameters as tuple or dict (optional)\n        as_dict: If True, return results as list of dicts instead of tuples\n\n    Returns:\n        List of result rows (as tuples or dicts based on as_dict parameter)\n        Returns empty list if database is not connected or query fails\n\n    Example:\n        # Fetch all products\n        results = fetch_all(\"SELECT * FROM products\")\n\n        # Fetch with positional parameters\n        results = fetch_all(\n            \"SELECT * FROM products WHERE category = %s\",\n            (\"electronics\",)\n        )\n\n        # Fetch with named parameters as dicts\n        results = fetch_all(\n            \"SELECT * FROM products WHERE price &gt; %(min_price)s\",\n            {\"min_price\": 100},\n            as_dict=True\n        )\n    \"\"\"\n    if not is_connected():\n        logger.warning(\"Database not connected, returning empty list\")\n        return []\n\n    conn = None\n    close_on_error = False\n    try:\n        conn = _get_connection()\n        cursor_factory = RealDictCursor if as_dict else None\n        with conn.cursor(cursor_factory=cursor_factory) as cur:\n            cur.execute(query, params)\n            result = cur.fetchall()\n            # Convert RealDictRow to regular dict for JSON serialization\n            if as_dict:\n                return [dict(row) for row in result]\n            return result\n    except (OperationalError, InterfaceError) as e:\n        # Connection error - mark for closing\n        close_on_error = True\n        logger.error(f\"Database connection error during query: {e}\")\n        return []\n    except psycopg2.DatabaseError as e:\n        logger.error(f\"Database query error: {e}\")\n        return []\n    except Exception as e:\n        logger.error(f\"Unexpected error during fetch_all: {e}\")\n        return []\n    finally:\n        _return_connection(conn, close_on_error)\n</code></pre>"},{"location":"api/backend/database/#backend.DB.fetch_all--fetch-all-products","title":"Fetch all products","text":"<p>results = fetch_all(\"SELECT * FROM products\")</p>"},{"location":"api/backend/database/#backend.DB.fetch_all--fetch-with-positional-parameters","title":"Fetch with positional parameters","text":"<p>results = fetch_all(     \"SELECT * FROM products WHERE category = %s\",     (\"electronics\",) )</p>"},{"location":"api/backend/database/#backend.DB.fetch_all--fetch-with-named-parameters-as-dicts","title":"Fetch with named parameters as dicts","text":"<p>results = fetch_all(     \"SELECT * FROM products WHERE price &gt; %(min_price)s\",     {\"min_price\": 100},     as_dict=True )</p>"},{"location":"api/backend/database/#backend.DB.fetch_one","title":"<code>fetch_one(query, params=None, as_dict=False)</code>","text":"<p>Execute a SELECT query and fetch a single result.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string (can use %s or %(name)s placeholders)</p> required <code>params</code> <code>Optional[Union[Tuple, Dict]]</code> <p>Query parameters as tuple or dict (optional)</p> <code>None</code> <code>as_dict</code> <code>bool</code> <p>If True, return result as dict instead of tuple</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Single result row (as tuple or dict) or None if no results or error</p> Example Source code in <code>backend\\DB.py</code> <pre><code>def fetch_one(\n    query: str,\n    params: Optional[Union[Tuple, Dict]] = None,\n    as_dict: bool = False\n) -&gt; Optional[Any]:\n    \"\"\"\n    Execute a SELECT query and fetch a single result.\n\n    Args:\n        query: SQL query string (can use %s or %(name)s placeholders)\n        params: Query parameters as tuple or dict (optional)\n        as_dict: If True, return result as dict instead of tuple\n\n    Returns:\n        Single result row (as tuple or dict) or None if no results or error\n\n    Example:\n        # Fetch single product by ID\n        product = fetch_one(\n            \"SELECT * FROM products WHERE id = %s\",\n            (123,),\n            as_dict=True\n        )\n    \"\"\"\n    if not is_connected():\n        logger.warning(\"Database not connected, returning None\")\n        return None\n\n    conn = None\n    close_on_error = False\n    try:\n        conn = _get_connection()\n        cursor_factory = RealDictCursor if as_dict else None\n        with conn.cursor(cursor_factory=cursor_factory) as cur:\n            cur.execute(query, params)\n            result = cur.fetchone()\n            # Convert RealDictRow to regular dict for JSON serialization\n            if as_dict and result is not None:\n                return dict(result)\n            return result\n    except (OperationalError, InterfaceError) as e:\n        # Connection error - mark for closing\n        close_on_error = True\n        logger.error(f\"Database connection error during query: {e}\")\n        return None\n    except psycopg2.DatabaseError as e:\n        logger.error(f\"Database query error: {e}\")\n        return None\n    except Exception as e:\n        logger.error(f\"Unexpected error during fetch_one: {e}\")\n        return None\n    finally:\n        _return_connection(conn, close_on_error)\n</code></pre>"},{"location":"api/backend/database/#backend.DB.fetch_one--fetch-single-product-by-id","title":"Fetch single product by ID","text":"<p>product = fetch_one(     \"SELECT * FROM products WHERE id = %s\",     (123,),     as_dict=True )</p>"},{"location":"api/backend/database/#backend.DB.close_pool","title":"<code>close_pool()</code>","text":"<p>Close all connections in the pool. Call this when shutting down the application. Safe to call multiple times.</p> Source code in <code>backend\\DB.py</code> <pre><code>def close_pool() -&gt; None:\n    \"\"\"\n    Close all connections in the pool.\n    Call this when shutting down the application.\n    Safe to call multiple times.\n    \"\"\"\n    global _db_pool\n    if _db_pool is not None:\n        try:\n            _db_pool.closeall()\n            logger.info(\"Database connection pool closed\")\n        except Exception as e:\n            logger.warning(f\"Error closing connection pool: {e}\")\n        finally:\n            _db_pool = None\n</code></pre>"},{"location":"api/backend/product-sku/","title":"Product SKU","text":"<p>Product SKU management \u2014 fetching product data from the backend API and local caching.</p>"},{"location":"api/backend/product-sku/#backend.get_product_sku","title":"<code>get_product_sku</code>","text":""},{"location":"api/backend/product-sku/#backend.get_product_sku--product-sku-api","title":"Product SKU API","text":"<p>Functions to fetch product SKU data from the database.</p>"},{"location":"api/backend/product-sku/#backend.get_product_sku-classes","title":"Classes","text":""},{"location":"api/backend/product-sku/#backend.get_product_sku.ProductSKUWorker","title":"<code>ProductSKUWorker(limit=None, parent=None)</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Qt Worker thread for fetching product SKU data. Use this for PySide6/Qt applications to avoid blocking the UI.</p> Signals <p>finished(list): Emitted with product data when complete error(str): Emitted with error message if query fails</p> Example <p>worker = ProductSKUWorker() worker.finished.connect(self.on_products_loaded) worker.error.connect(self.on_error) worker.start()</p> <p>def on_products_loaded(self, products):     for product in products:         print(product['default_code'])</p> Source code in <code>backend\\get_product_sku.py</code> <pre><code>def __init__(\n    self,\n    limit: Optional[int] = None,\n    parent: Optional[QObject] = None\n):\n    super().__init__(parent)\n    self.limit = limit\n</code></pre>"},{"location":"api/backend/product-sku/#backend.get_product_sku-functions","title":"Functions","text":""},{"location":"api/backend/product-sku/#backend.get_product_sku.get_product_sku","title":"<code>get_product_sku(limit=None)</code>","text":"<p>Fetch product SKU data from the database (synchronous).</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of results</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of product dictionaries with keys: 'Nama Produk', 'Perbesaran Ukuran (Otorisasi)', </p> <code>List[Dict[str, Any]]</code> <p>'List Size Available', 'Kategori', 'GDrive ID'.</p> <code>List[Dict[str, Any]]</code> <p>Returns empty list if database is not connected or query fails.</p> Example <p>products = get_product_sku()</p> Source code in <code>backend\\get_product_sku.py</code> <pre><code>def get_product_sku(\n    limit: Optional[int] = None\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Fetch product SKU data from the database (synchronous).\n\n    Args:\n        limit: Optional limit on number of results\n\n    Returns:\n        List of product dictionaries with keys: 'Nama Produk', 'Perbesaran Ukuran (Otorisasi)', \n        'List Size Available', 'Kategori', 'GDrive ID'.\n        Returns empty list if database is not connected or query fails.\n\n    Example:\n        products = get_product_sku()\n        # [{'Nama Produk': 'Sandal X', 'GDrive ID': '1abc...', ...}, ...]\n    \"\"\"\n    if not is_connected():\n        return []\n\n    # Build query with optional LIMIT\n    limit_clause = f\"LIMIT {int(limit)}\" if limit else \"\"\n\n    query = f'''\n    WITH base_data AS (\n    SELECT\n        ptd.product_code,\n        ptd.revisi,\n        pm.name AS divisi,\n        ptd.size,\n        ptd.code_cetakan,\n        ptd.normal_size AS perbesar_ukuran,\n--         ptd.hardness,\n        ptd.state,\n        MAX(\n            CASE\n                WHEN ir.is_cover = true\n                THEN concat('https://lh3.googleusercontent.com/d/', ir.gdrive_id, '=s600')\n            END\n        ) AS cover_image\n    FROM product_template_dev ptd\n        LEFT JOIN product_template pt ON pt.id = ptd.product_tmpl_id\n        LEFT JOIN product_product pp ON pp.product_tmpl_id = pt.id\n        LEFT JOIN product_dev_plant_rel pdp ON pdp.product_dev_id = ptd.id\n        LEFT JOIN plant_master pm ON pm.id = pdp.plant_id\n        LEFT JOIN crm_team ct ON ct.id = ptd.team_id\n        LEFT JOIN product_attribute_value_product_product_rel rel_warna\n            ON rel_warna.product_product_id = pp.id\n        LEFT JOIN product_attribute_value pav\n            ON pav.id = rel_warna.product_attribute_value_id\n        LEFT JOIN product_template_dev_line ptdl ON ptdl.product_dev_id = ptd.id\n        LEFT JOIN ir_attachment ir\n            ON ptd.id = ir.res_id\n           AND ir.res_model::text = 'product.template.dev'\n    WHERE\n        pav.attribute_id = 2\n        AND ptd.state &lt;&gt; 'cancel'\n        AND ptd.active = true\n        AND ptd.size IS NOT NULL\n        AND pm.name = 'EVA1'\n        AND lower(COALESCE(ptd.product_code, '')) NOT LIKE '%label%'\n        AND lower(COALESCE(ptd.product_code, '')) NOT LIKE '%aksesoris%'\n    GROUP BY\n        ptd.id, ptd.name, ptd.otorisasi_type, ptd.is_portolady,\n        pt.id, ptd.product_code, ptd.document_code, ptd.revisi,\n        pm.name, ptd.size, ptd.release_date, ptd.otorisasi_date,\n        ptd.is_registered_haki, ptd.haki_submit_date, ptd.code_cetakan,\n        ptd.normal_size, ptd.is_release_exdig, ct.name, ptd.is_no_brand,\n        ptd.notes_tali, ptd.notes_accessories, ptd.hardness,\n        ptd.notes_packing, ptd.is_alternative_acc, ptd.state\n)\n, ranked_data AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY product_code\n            ORDER BY revisi DESC\n        ) AS rn\n    FROM base_data\n)\nSELECT *\nFROM ranked_data\nWHERE rn = 1\n    {limit_clause}\n    '''\n\n    result = fetch_all(query, as_dict=True)\n\n    # Process and clean data\n    cleaned_result = []\n    if result:\n        for row in result:\n            # Parse Otorisasi (Perbesaran Ukuran)\n            # Format examples: \"+1.0\", \"+0.5\", \"pas\", \"Pas\", \"0\"\n            raw_oto = str(row.get('perbesaran_ukuran', '')).lower().strip()\n            oto_val = 0.0\n            if raw_oto and raw_oto not in ['pas', 'none', 'null']:\n                try:\n                    oto_val = float(raw_oto.replace('+', ''))\n                except ValueError:\n                    pass\n\n            # Map to clean dictionary\n            cleaned_result.append({\n                'Nama Produk': row.get('product_code', 'Unknown'),\n                'Perbesaran Ukuran (Otorisasi)': oto_val,  # Now a float\n                'Raw Otorisasi': row.get('perbesaran_ukuran'), # Keep original just in case\n                'List Size Available': row.get('size', ''),\n                'Kategori': row.get('divisi', 'Unknown'),\n                'Cover Image': row.get('cover_image'), # Direct URL\n                'GDrive ID': row.get('cover_image') # Legacy support (aliased to URL)\n            })\n\n    return cleaned_result\n</code></pre>"},{"location":"api/backend/product-sku/#backend.get_product_sku.get_product_sku--nama-produk-sandal-x-gdrive-id-1abc","title":"[{'Nama Produk': 'Sandal X', 'GDrive ID': '1abc...', ...}, ...]","text":""},{"location":"api/backend/product-sku/#backend.get_product_sku.get_product_sku_json","title":"<code>get_product_sku_json(limit=None)</code>","text":"<p>Fetch product SKU data and return as JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of results</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON string of product data</p> Source code in <code>backend\\get_product_sku.py</code> <pre><code>def get_product_sku_json(\n    limit: Optional[int] = None\n) -&gt; str:\n    \"\"\"\n    Fetch product SKU data and return as JSON string.\n\n    Args:\n        limit: Optional limit on number of results\n\n    Returns:\n        JSON string of product data\n    \"\"\"\n    data = get_product_sku(limit=limit)\n    return json.dumps(data, ensure_ascii=False, indent=2)\n</code></pre>"},{"location":"api/backend/product-sku/#backend.get_product_sku.get_product_sku_async","title":"<code>get_product_sku_async(callback, error_callback=None, limit=None)</code>","text":"<p>Fetch product SKU data asynchronously using a background thread. Non-blocking - returns immediately while query runs in background.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable[[List[Dict[str, Any]]], None]</code> <p>Function to call with results when complete</p> required <code>error_callback</code> <code>Optional[Callable[[Exception], None]]</code> <p>Optional function to call if an error occurs</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of results</p> <code>None</code> <p>Returns:</p> Type Description <code>Thread</code> <p>The thread object (already started)</p> Example <p>def on_products_loaded(products):     print(f\"Loaded {len(products)} products\")     for p in products:         print(p['default_code'])</p> <p>def on_error(e):     print(f\"Error: {e}\")</p> <p>get_product_sku_async(     callback=on_products_loaded,     error_callback=on_error )</p> Source code in <code>backend\\get_product_sku.py</code> <pre><code>def get_product_sku_async(\n    callback: Callable[[List[Dict[str, Any]]], None],\n    error_callback: Optional[Callable[[Exception], None]] = None,\n    limit: Optional[int] = None\n) -&gt; threading.Thread:\n    \"\"\"\n    Fetch product SKU data asynchronously using a background thread.\n    Non-blocking - returns immediately while query runs in background.\n\n    Args:\n        callback: Function to call with results when complete\n        error_callback: Optional function to call if an error occurs\n        limit: Optional limit on number of results\n\n    Returns:\n        The thread object (already started)\n\n    Example:\n        def on_products_loaded(products):\n            print(f\"Loaded {len(products)} products\")\n            for p in products:\n                print(p['default_code'])\n\n        def on_error(e):\n            print(f\"Error: {e}\")\n\n        get_product_sku_async(\n            callback=on_products_loaded,\n            error_callback=on_error\n        )\n    \"\"\"\n    def run():\n        try:\n            result = get_product_sku(limit=limit)\n            callback(result)\n        except Exception as e:\n            if error_callback:\n                error_callback(e)\n\n    thread = threading.Thread(target=run, daemon=True)\n    thread.start()\n    return thread\n</code></pre>"},{"location":"api/backend/product-sku/#backend.get_product_sku.get_product_sku_worker","title":"<code>get_product_sku_worker(limit=None, parent=None)</code>","text":"<p>Create a Qt worker thread for fetching product SKU data. Convenience function that returns an unstarted worker.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of results</p> <code>None</code> <code>parent</code> <code>Optional[QObject]</code> <p>Optional Qt parent object</p> <code>None</code> <p>Returns:</p> Type Description <code>ProductSKUWorker</code> <p>ProductSKUWorker instance (not started - call .start())</p> Example <p>worker = get_product_sku_worker(limit=100) worker.finished.connect(lambda data: print(f\"Got {len(data)} products\")) worker.error.connect(lambda err: print(f\"Error: {err}\")) worker.start()</p> Source code in <code>backend\\get_product_sku.py</code> <pre><code>def get_product_sku_worker(\n    limit: Optional[int] = None,\n    parent: Optional[QObject] = None\n) -&gt; ProductSKUWorker:\n    \"\"\"\n    Create a Qt worker thread for fetching product SKU data.\n    Convenience function that returns an unstarted worker.\n\n    Args:\n        limit: Optional limit on number of results\n        parent: Optional Qt parent object\n\n    Returns:\n        ProductSKUWorker instance (not started - call .start())\n\n    Example:\n        worker = get_product_sku_worker(limit=100)\n        worker.finished.connect(lambda data: print(f\"Got {len(data)} products\"))\n        worker.error.connect(lambda err: print(f\"Error: {err}\"))\n        worker.start()\n    \"\"\"\n    return ProductSKUWorker(limit=limit, parent=parent)\n</code></pre>"},{"location":"api/backend/size-categorization/","title":"Size Categorization","text":"<p>Utility module for categorizing measurements into size classes (S, M, L, XL, etc.) based on product profiles.</p>"},{"location":"api/backend/size-categorization/#backend.size_categorization","title":"<code>size_categorization</code>","text":""},{"location":"api/backend/size-categorization/#backend.size_categorization--size-based-categorization-utility","title":"Size-Based Categorization Utility","text":"<p>Categorizes sandal measurements into GOOD, OVEN, or REJECT based on deviation from target size.</p> <p>Business Logic: - Target Size = Selected Size + Otorisasi - Target Length (cm) = Target Size \u00d7 (2/3) - Deviation is measured in \"size units\" (1 size unit = 2/3 cm \u2248 6.67 mm)</p> <p>Categories: - GOOD 1: 0 to +0.25 size units - GOOD 2: +0.25 to +0.5 size units - OVEN 1: +0.5 to +1.0 size units - OVEN 2: +1.0 to +1.5 size units - REJECT: Below 0 OR Above +1.5 size units</p>"},{"location":"api/backend/size-categorization/#backend.size_categorization-functions","title":"Functions","text":""},{"location":"api/backend/size-categorization/#backend.size_categorization.categorize_measurement","title":"<code>categorize_measurement(measured_length_mm, selected_size, otorisasi=0.0)</code>","text":"<p>Categorize a sandal measurement based on size deviation.</p> <p>Parameters:</p> Name Type Description Default <code>measured_length_mm</code> <code>float</code> <p>The measured length in millimeters.</p> required <code>selected_size</code> <code>float</code> <p>The selected size (e.g., 40).</p> required <code>otorisasi</code> <code>float</code> <p>The authorization scaling factor (e.g., +1).</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with:</p> <code>Dict[str, Any]</code> <ul> <li>category: \"GOOD\", \"OVEN\", or \"REJECT\"</li> </ul> <code>Dict[str, Any]</code> <ul> <li>detail: \"GOOD 1\", \"GOOD 2\", \"OVEN 1\", \"OVEN 2\", or \"REJECT\"</li> </ul> <code>Dict[str, Any]</code> <ul> <li>target_size: The calculated target size (selected + otorisasi)</li> </ul> <code>Dict[str, Any]</code> <ul> <li>target_length_mm: The target length in mm</li> </ul> <code>Dict[str, Any]</code> <ul> <li>measured_size: The measured length converted to size units</li> </ul> <code>Dict[str, Any]</code> <ul> <li>deviation_size: The deviation from target in size units</li> </ul> <code>Dict[str, Any]</code> <ul> <li>deviation_mm: The deviation from target in mm</li> </ul> Source code in <code>backend\\size_categorization.py</code> <pre><code>def categorize_measurement(\n    measured_length_mm: float,\n    selected_size: float,\n    otorisasi: float = 0.0\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Categorize a sandal measurement based on size deviation.\n\n    Args:\n        measured_length_mm: The measured length in millimeters.\n        selected_size: The selected size (e.g., 40).\n        otorisasi: The authorization scaling factor (e.g., +1).\n\n    Returns:\n        Dictionary with:\n        - category: \"GOOD\", \"OVEN\", or \"REJECT\"\n        - detail: \"GOOD 1\", \"GOOD 2\", \"OVEN 1\", \"OVEN 2\", or \"REJECT\"\n        - target_size: The calculated target size (selected + otorisasi)\n        - target_length_mm: The target length in mm\n        - measured_size: The measured length converted to size units\n        - deviation_size: The deviation from target in size units\n        - deviation_mm: The deviation from target in mm\n    \"\"\"\n    # Calculate target\n    target_size = selected_size + otorisasi\n    target_length_cm = target_size * (2 / 3)\n    target_length_mm = target_length_cm * 10  # Convert to mm\n\n    # Convert measured length to size units\n    measured_length_cm = measured_length_mm / 10\n    measured_size = measured_length_cm * (3 / 2)  # Inverse of 2/3\n\n    # Calculate deviation in size units\n    deviation_size = measured_size - target_size\n    deviation_mm = measured_length_mm - target_length_mm\n\n    # Categorize based on deviation\n    if deviation_size &lt; 0:\n        category = \"REJECT\"\n        detail = \"REJECT (UNDER)\"\n    elif deviation_size &lt;= 0.25:\n        category = \"GOOD\"\n        detail = \"GOOD 1\"\n    elif deviation_size &lt;= 0.5:\n        category = \"GOOD\"\n        detail = \"GOOD 2\"\n    elif deviation_size &lt;= 1.0:\n        category = \"OVEN\"\n        detail = \"OVEN 1\"\n    elif deviation_size &lt;= 1.5:\n        category = \"OVEN\"\n        detail = \"OVEN 2\"\n    else:\n        category = \"REJECT\"\n        detail = \"REJECT (OVER)\"\n\n    return {\n        \"category\": category,\n        \"detail\": detail,\n        \"target_size\": target_size,\n        \"target_length_mm\": round(target_length_mm, 2),\n        \"measured_size\": round(measured_size, 2),\n        \"deviation_size\": round(deviation_size, 4),\n        \"deviation_mm\": round(deviation_mm, 2)\n    }\n</code></pre>"},{"location":"api/backend/size-categorization/#backend.size_categorization.get_category_color","title":"<code>get_category_color(category)</code>","text":"<p>Get the display color for a category.</p> <p>Returns:</p> Type Description <code>str</code> <p>Hex color code for UI display.</p> Source code in <code>backend\\size_categorization.py</code> <pre><code>def get_category_color(category: str) -&gt; str:\n    \"\"\"\n    Get the display color for a category.\n\n    Returns:\n        Hex color code for UI display.\n    \"\"\"\n    colors = {\n        \"GOOD\": \"#4CAF50\",   # Green\n        \"OVEN\": \"#FF9800\",   # Orange\n        \"REJECT\": \"#D32F2F\"  # Red\n    }\n    return colors.get(category, \"#999999\")\n</code></pre>"},{"location":"api/model/advanced-inference/","title":"advanced_inference","text":"<p>Advanced two-model segmentation pipeline using YOLOv8x + SAM.</p> <ul> <li>The Spotter (YOLOv8x): Extra-Large detection model that finds bounding boxes using texture differences</li> <li>The Surgeon (SAM): Segment Anything Model that produces pixel-perfect masks from edge &amp; depth cues</li> </ul>"},{"location":"api/model/advanced-inference/#model.advanced_inference","title":"<code>advanced_inference</code>","text":"<p>Advanced Inference Module \u2014 YOLOv8-X + SAM Pipeline</p> Two-model architecture <ol> <li>The Spotter (YOLOv8-X): Finds the bounding box. The Extra Large model    is smart enough to see texture differences even when colors are identical.</li> <li>The Surgeon (SAM): Pixel-perfect segmentation within that bounding box.    Uses edge and depth cues \u2014 doesn't care about class labels or color.</li> </ol>"},{"location":"api/model/advanced-inference/#model.advanced_inference-functions","title":"Functions","text":""},{"location":"api/model/advanced-inference/#model.advanced_inference.get_yolo_model","title":"<code>get_yolo_model()</code>","text":"<p>Load YOLOv8-X detection model (lazy loading with caching). Downloads ~130MB model on first use.</p> Source code in <code>model\\advanced_inference.py</code> <pre><code>def get_yolo_model():\n    \"\"\"\n    Load YOLOv8-X detection model (lazy loading with caching).\n    Downloads ~130MB model on first use.\n    \"\"\"\n    global _yolo_model\n\n    if _yolo_model is not None:\n        return _yolo_model\n\n    try:\n        from ultralytics import YOLO\n\n        model_name = \"yolov8x.pt\"\n        print(f\"[Advanced] Loading YOLOv8-X (The Spotter): {model_name}\")\n\n        _yolo_model = YOLO(model_name)\n\n        print(\"[Advanced] YOLOv8-X loaded successfully\")\n        return _yolo_model\n\n    except ImportError:\n        print(\"[Advanced] Error: ultralytics not installed. Run: pip install ultralytics\")\n        return None\n    except Exception as e:\n        print(f\"[Advanced] Error loading YOLOv8-X: {e}\")\n        return None\n</code></pre>"},{"location":"api/model/advanced-inference/#model.advanced_inference.get_sam_model","title":"<code>get_sam_model()</code>","text":"<p>Load SAM (Segment Anything Model) via Ultralytics (lazy loading with caching). Downloads ~375MB model on first use.</p> Source code in <code>model\\advanced_inference.py</code> <pre><code>def get_sam_model():\n    \"\"\"\n    Load SAM (Segment Anything Model) via Ultralytics (lazy loading with caching).\n    Downloads ~375MB model on first use.\n    \"\"\"\n    global _sam_model\n\n    if _sam_model is not None:\n        return _sam_model\n\n    try:\n        from ultralytics import SAM\n\n        model_name = \"sam_b.pt\"\n        print(f\"[Advanced] Loading SAM (The Surgeon): {model_name}\")\n\n        _sam_model = SAM(model_name)\n\n        print(\"[Advanced] SAM loaded successfully\")\n        return _sam_model\n\n    except ImportError:\n        print(\"[Advanced] Error: ultralytics not installed. Run: pip install ultralytics\")\n        return None\n    except Exception as e:\n        print(f\"[Advanced] Error loading SAM: {e}\")\n        return None\n</code></pre>"},{"location":"api/model/advanced-inference/#model.advanced_inference.segment_image","title":"<code>segment_image(image, conf=0.25)</code>","text":"<p>Advanced two-model segmentation pipeline.</p> <ol> <li>YOLOv8-X detects objects \u2192 bounding box</li> <li>SAM segments inside that box \u2192 pixel-perfect mask</li> </ol> <p>Parameters:</p> Name Type Description Default <code>image</code> <p>BGR image (numpy array) or path to image</p> required <code>conf</code> <p>Confidence threshold for YOLOv8-X detection</p> <code>0.25</code> <p>Returns:</p> Name Type Description <code>mask</code> <p>Binary mask of the best detected object (numpy array, 0/255)</p> <code>contour</code> <p>Largest contour of the segmented object</p> <code>inference_time</code> <p>Total time for both models in milliseconds</p> Source code in <code>model\\advanced_inference.py</code> <pre><code>def segment_image(image, conf=0.25):\n    \"\"\"\n    Advanced two-model segmentation pipeline.\n\n    1. YOLOv8-X detects objects \u2192 bounding box\n    2. SAM segments inside that box \u2192 pixel-perfect mask\n\n    Args:\n        image: BGR image (numpy array) or path to image\n        conf: Confidence threshold for YOLOv8-X detection\n\n    Returns:\n        mask: Binary mask of the best detected object (numpy array, 0/255)\n        contour: Largest contour of the segmented object\n        inference_time: Total time for both models in milliseconds\n    \"\"\"\n    import time\n\n    yolo_model = get_yolo_model()\n    sam_model = get_sam_model()\n\n    if yolo_model is None or sam_model is None:\n        print(\"[Advanced] Error: One or both models failed to load\")\n        return None, None, 0\n\n    # Load image if path is provided\n    if isinstance(image, str):\n        img = cv2.imread(image)\n    else:\n        img = image\n\n    if img is None:\n        print(\"[Advanced] Error: Could not load image\")\n        return None, None, 0\n\n    h, w = img.shape[:2]\n    img_area = h * w\n    img_center = (w / 2, h / 2)\n\n    start_time = time.time()\n\n    # ======================================================\n    # STEP 1: THE SPOTTER \u2014 YOLOv8-X finds the bounding box\n    # ======================================================\n    print(\"[Advanced] Step 1: Running YOLOv8-X detection...\")\n\n    yolo_results = yolo_model(\n        img,\n        device=_get_device(),\n        conf=conf,\n        verbose=False\n    )\n\n    yolo_time = (time.time() - start_time) * 1000\n    print(f\"[Advanced] YOLOv8-X inference: {yolo_time:.0f}ms\")\n\n    best_box = None\n\n    if len(yolo_results) == 0 or yolo_results[0].boxes is None or len(yolo_results[0].boxes) == 0:\n        print(\"[Advanced] YOLOv8-X found no objects\")\n    else:\n        boxes = yolo_results[0].boxes.xyxy.cpu().numpy()\n        confidences = yolo_results[0].boxes.conf.cpu().numpy()\n\n        print(f\"[Advanced] YOLOv8-X found {len(boxes)} objects, selecting best one...\")\n\n        # === Select the best bounding box ===\n        best_score = -1\n\n        for i, (box, box_conf) in enumerate(zip(boxes, confidences)):\n            x1, y1, x2, y2 = box\n            box_w = x2 - x1\n            box_h = y2 - y1\n            box_area = box_w * box_h\n            area_ratio = box_area / img_area\n\n            # Skip background-sized or noise-sized detections\n            if area_ratio &gt; 0.80:\n                continue\n            if area_ratio &lt; 0.01:\n                continue\n\n            # Center score: prefer objects near image center\n            cx = (x1 + x2) / 2\n            cy = (y1 + y2) / 2\n            center_dist = np.sqrt((cx - img_center[0])**2 + (cy - img_center[1])**2)\n            max_dist = np.sqrt(img_center[0]**2 + img_center[1]**2)\n            center_score = 1 - (center_dist / max_dist)\n\n            # Combined score: area + center + confidence\n            score = (area_ratio * 0.4) + (center_score * 0.3) + (float(box_conf) * 0.3)\n\n            print(f\"[Advanced]   Box {i}: area={area_ratio*100:.1f}%, center={center_score:.2f}, \"\n                  f\"conf={box_conf:.2f}, score={score:.3f}\")\n\n            if score &gt; best_score:\n                best_score = score\n                best_box = box\n\n    # ======================================================\n    # STEP 2: THE SURGEON \u2014 SAM segments the object\n    # ======================================================\n    sam_start = time.time()\n\n    if best_box is not None:\n        # === Normal path: YOLO found a box \u2192 feed it to SAM ===\n        x1, y1, x2, y2 = best_box\n        box_w = x2 - x1\n        box_h = y2 - y1\n        margin = 0.10\n        x1_exp = max(0, int(x1 - box_w * margin))\n        y1_exp = max(0, int(y1 - box_h * margin))\n        x2_exp = min(w, int(x2 + box_w * margin))\n        y2_exp = min(h, int(y2 + box_h * margin))\n\n        print(f\"[Advanced] Step 2: Running SAM with bounding box [{x1_exp}, {y1_exp}, {x2_exp}, {y2_exp}]...\")\n\n        sam_results = sam_model(\n            img,\n            bboxes=[x1_exp, y1_exp, x2_exp, y2_exp],\n            verbose=False\n        )\n    else:\n        # === Fallback path: YOLO is blind \u2192 prompt SAM with center point ===\n        # SAM uses edges &amp; texture, NOT color \u2014 it can see green-on-green\n        center_x = int(w / 2)\n        center_y = int(h / 2)\n\n        print(f\"[Advanced] Step 2: YOLO blind \u2014 Running SAM with center point ({center_x}, {center_y})...\")\n\n        sam_results = sam_model(\n            img,\n            points=[[center_x, center_y]],\n            labels=[1],  # 1 = foreground point\n            verbose=False\n        )\n\n    sam_time = (time.time() - sam_start) * 1000\n    print(f\"[Advanced] SAM inference: {sam_time:.0f}ms\")\n\n    if len(sam_results) == 0 or sam_results[0].masks is None or len(sam_results[0].masks) == 0:\n        print(\"[Advanced] SAM produced no masks, falling back to bounding box crop\")\n        total_time = (time.time() - start_time) * 1000\n        return None, None, total_time\n\n    # SAM may return multiple masks \u2014 pick the one with the largest area\n    sam_masks = sam_results[0].masks.data.cpu().numpy()\n    print(f\"[Advanced] SAM produced {len(sam_masks)} mask(s)\")\n\n    best_sam_mask = None\n    best_sam_area = 0\n\n    for i, mask in enumerate(sam_masks):\n        mask_resized = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n        mask_area = np.sum(mask_resized &gt; 0)\n        area_ratio = mask_area / img_area\n\n        # Skip if the mask covers basically the entire image (background)\n        if area_ratio &gt; 0.85:\n            print(f\"[Advanced]   SAM Mask {i}: {area_ratio*100:.1f}% \u2014 skipped (background)\")\n            continue\n\n        if mask_area &gt; best_sam_area:\n            best_sam_area = mask_area\n            best_sam_mask = mask_resized\n\n    if best_sam_mask is None:\n        # If all masks were too large, just use the first one\n        print(\"[Advanced] All SAM masks were large, using first mask\")\n        best_sam_mask = cv2.resize(sam_masks[0].astype(np.uint8), (w, h),\n                                   interpolation=cv2.INTER_NEAREST)\n\n    # Convert to binary mask (0 or 255)\n    mask_binary = (best_sam_mask * 255).astype(np.uint8)\n\n    # Light morphological cleanup (SAM is already precise, minimal cleanup)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n    mask_binary = cv2.morphologyEx(mask_binary, cv2.MORPH_CLOSE, kernel, iterations=1)\n    mask_binary = cv2.morphologyEx(mask_binary, cv2.MORPH_OPEN, kernel, iterations=1)\n\n    # === Extract Final Contour ===\n    contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    if len(contours) == 0:\n        print(\"[Advanced] No contour found in SAM mask\")\n        total_time = (time.time() - start_time) * 1000\n        return mask_binary, None, total_time\n\n    largest_contour = max(contours, key=cv2.contourArea)\n\n    total_time = (time.time() - start_time) * 1000\n\n    final_area = cv2.contourArea(largest_contour)\n    final_ratio = final_area / img_area\n    print(f\"[Advanced] Final mask: {final_ratio*100:.1f}% of image\")\n    print(f\"[Advanced] Total pipeline: {total_time:.0f}ms (YOLO: {yolo_time:.0f}ms + SAM: {sam_time:.0f}ms)\")\n\n    return mask_binary, largest_contour, total_time\n</code></pre>"},{"location":"api/model/advanced-inference/#model.advanced_inference.get_mask_and_contour","title":"<code>get_mask_and_contour(image_path)</code>","text":"<p>Convenience function matching the interface of other inference modules.</p> Source code in <code>model\\advanced_inference.py</code> <pre><code>def get_mask_and_contour(image_path):\n    \"\"\"Convenience function matching the interface of other inference modules.\"\"\"\n    mask, contour, inference_time = segment_image(image_path)\n    return contour, mask, inference_time\n</code></pre>"},{"location":"api/model/fastsam-inference/","title":"fastsam_inference","text":"<p>FastSAM (Fast Segment Anything Model) inference module for universal object segmentation.</p> <p>Uses <code>FastSAM-s.pt</code> (~24 MB) \u2014 the smaller variant optimized for speed.</p>"},{"location":"api/model/fastsam-inference/#model.fastsam_inference","title":"<code>fastsam_inference</code>","text":"<p>FastSAM Inference Module</p> <p>Lightweight AI-based segmentation using FastSAM (Fast Segment Anything Model). Runs on CPU and provides better edge detection than traditional contour methods.</p>"},{"location":"api/model/fastsam-inference/#model.fastsam_inference-functions","title":"Functions","text":""},{"location":"api/model/fastsam-inference/#model.fastsam_inference.get_model","title":"<code>get_model()</code>","text":"<p>Load FastSAM model (lazy loading with caching). Downloads ~23MB model on first use.</p> Source code in <code>model\\fastsam_inference.py</code> <pre><code>def get_model():\n    \"\"\"\n    Load FastSAM model (lazy loading with caching).\n    Downloads ~23MB model on first use.\n    \"\"\"\n    global _model, _model_path\n\n    if _model is not None:\n        return _model\n\n    try:\n        from ultralytics import YOLO\n\n        # FastSAM-s is the smaller variant (~23MB)\n        model_name = \"FastSAM-s.pt\"\n\n        # Store in model folder\n        model_dir = os.path.dirname(os.path.abspath(__file__))\n        _model_path = os.path.join(model_dir, model_name)\n\n        print(f\"[FastSAM] Loading model: {model_name}\")\n\n        # YOLO will auto-download if not present\n        _model = YOLO(model_name)\n\n        print(\"[FastSAM] Model loaded successfully\")\n        return _model\n\n    except ImportError:\n        print(\"[FastSAM] Error: ultralytics not installed. Run: pip install ultralytics\")\n        return None\n    except Exception as e:\n        print(f\"[FastSAM] Error loading model: {e}\")\n        return None\n</code></pre>"},{"location":"api/model/fastsam-inference/#model.fastsam_inference.is_available","title":"<code>is_available()</code>","text":"<p>Check if FastSAM dependencies are available.</p> Source code in <code>model\\fastsam_inference.py</code> <pre><code>def is_available():\n    \"\"\"Check if FastSAM dependencies are available.\"\"\"\n    try:\n        from ultralytics import YOLO\n        return True\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/model/fastsam-inference/#model.fastsam_inference.segment_image","title":"<code>segment_image(image, conf=0.4, iou=0.9)</code>","text":"<p>Segment the image using FastSAM.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <p>BGR image (numpy array) or path to image</p> required <code>conf</code> <p>Confidence threshold (default 0.4 to reduce shadow/background noise)</p> <code>0.4</code> <code>iou</code> <p>IoU threshold for NMS (default 0.9)</p> <code>0.9</code> <p>Returns:</p> Name Type Description <code>mask</code> <p>Binary mask of the largest detected object (numpy array)</p> <code>contour</code> <p>Contour of the largest object</p> <code>inference_time</code> <p>Time taken for inference in milliseconds</p> Source code in <code>model\\fastsam_inference.py</code> <pre><code>def segment_image(image, conf=0.4, iou=0.9):\n    \"\"\"\n    Segment the image using FastSAM.\n\n    Args:\n        image: BGR image (numpy array) or path to image\n        conf: Confidence threshold (default 0.4 to reduce shadow/background noise)\n        iou: IoU threshold for NMS (default 0.9)\n\n    Returns:\n        mask: Binary mask of the largest detected object (numpy array)\n        contour: Contour of the largest object\n        inference_time: Time taken for inference in milliseconds\n    \"\"\"\n    import time\n\n    model = get_model()\n    if model is None:\n        return None, None, 0\n\n    # Load image if path is provided\n    if isinstance(image, str):\n        img = cv2.imread(image)\n    else:\n        img = image\n\n    if img is None:\n        print(\"[FastSAM] Error: Could not load image\")\n        return None, None, 0\n\n    h, w = img.shape[:2]\n    img_area = h * w\n    img_center = (w / 2, h / 2)\n\n    start_time = time.time()\n\n    # Calculate optimal inference size (preserve detail)\n    # Use the largest dimension, capped at 1920 to prevent memory issues\n    # Must be a multiple of 32 for YOLO/FastSAM\n    max_dim = max(h, w)\n    inference_size = min(max_dim, 1920) \n    # Round to nearest multiple of 32\n    inference_size = int(round(inference_size / 32) * 32)\n\n    print(f\"[FastSAM] Running inference at size {inference_size}x{inference_size} (Original: {w}x{h})\")\n\n    # Run inference\n    results = model(\n        img,\n        device='cuda' if __import__('torch').cuda.is_available() else 'cpu',\n        retina_masks=True,\n        imgsz=inference_size,\n        conf=conf,\n        iou=iou,\n        verbose=False\n    )\n\n    inference_time = (time.time() - start_time) * 1000  # Convert to ms\n\n    if len(results) == 0 or results[0].masks is None:\n        print(\"[FastSAM] No objects detected\")\n        return None, None, inference_time\n\n    # Get all masks\n    masks = results[0].masks.data.cpu().numpy()\n\n    if len(masks) == 0:\n        print(\"[FastSAM] No masks found\")\n        return None, None, inference_time\n\n    print(f\"[FastSAM] Found {len(masks)} masks, selecting best one...\")\n\n    # Find the best mask (not just the largest!)\n    # We want: \n    # 1. Not too large (&gt;70% of image = likely background)\n    # 2. Not too small (&lt;1% of image = noise)\n    # 3. Prefer masks centered in the image\n\n    best_mask = None\n    best_score = -1\n\n    for i, mask in enumerate(masks):\n        # Resize mask to image dimensions\n        mask_resized = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n\n        # Calculate mask area ratio\n        mask_area = np.sum(mask_resized &gt; 0)\n        area_ratio = mask_area / img_area\n\n        # Skip if too large (background) or too small (noise)\n        if area_ratio &gt; 0.70:\n            print(f\"[FastSAM]   Mask {i}: {area_ratio*100:.1f}% - skipped (too large, likely background)\")\n            continue\n        if area_ratio &lt; 0.01:\n            print(f\"[FastSAM]   Mask {i}: {area_ratio*100:.1f}% - skipped (too small)\")\n            continue\n\n        # Calculate mask centroid\n        moments = cv2.moments(mask_resized)\n        if moments[\"m00\"] &gt; 0:\n            cx = moments[\"m10\"] / moments[\"m00\"]\n            cy = moments[\"m01\"] / moments[\"m00\"]\n\n            # Distance from center (normalized)\n            center_dist = np.sqrt((cx - img_center[0])**2 + (cy - img_center[1])**2)\n            max_dist = np.sqrt(img_center[0]**2 + img_center[1]**2)\n            center_score = 1 - (center_dist / max_dist)  # Higher = more centered\n        else:\n            center_score = 0\n\n        # Score: prefer larger masks that are centered\n        # area_ratio is 0-0.7, center_score is 0-1\n        score = (area_ratio * 0.7) + (center_score * 0.3)\n\n        print(f\"[FastSAM]   Mask {i}: {area_ratio*100:.1f}% area, center_score={center_score:.2f}, score={score:.3f}\")\n\n        if score &gt; best_score:\n            best_score = score\n            best_mask = mask_resized\n\n    if best_mask is None:\n        print(\"[FastSAM] No suitable mask found, using largest non-background mask\")\n        # Fallback: use the largest mask that's not the background\n        for mask in masks:\n            mask_resized = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n            mask_area = np.sum(mask_resized &gt; 0)\n            area_ratio = mask_area / img_area\n            if area_ratio &lt; 0.85:  # Accept anything smaller than 85%\n                best_mask = mask_resized\n                break\n\n    if best_mask is None:\n        print(\"[FastSAM] Could not find suitable object mask\")\n        return None, None, inference_time\n\n    # Convert to binary mask (0 or 255)\n    mask_binary = (best_mask * 255).astype(np.uint8)\n\n    # Apply edge refinement (hybrid: AI detection + Standard edge precision)\n    print(\"[FastSAM] Applying edge refinement for curved boundaries...\")\n    mask_binary = refine_ai_mask_with_edges(img, mask_binary)\n\n    # Extract contour from refined mask\n    contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    if len(contours) == 0:\n        return mask_binary, None, inference_time\n\n    # Get the largest contour\n    largest_contour = max(contours, key=cv2.contourArea)\n\n    # Smooth the contour using approxPolyDP for cleaner curved edges\n    epsilon = 0.002 * cv2.arcLength(largest_contour, True)\n    smoothed_contour = cv2.approxPolyDP(largest_contour, epsilon, True)\n\n    print(f\"[FastSAM] Segmentation complete in {inference_time:.1f}ms\")\n\n    return mask_binary, smoothed_contour, inference_time\n</code></pre>"},{"location":"api/model/fastsam-inference/#model.fastsam_inference.get_mask_and_contour","title":"<code>get_mask_and_contour(image_path)</code>","text":"<p>Convenience function to get mask and contour for measurement.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <p>Path to the image file</p> required <p>Returns:</p> Name Type Description <code>contour</code> <p>Largest object contour (for measurement)</p> <code>mask</code> <p>Binary mask</p> <code>inference_time</code> <p>Time in milliseconds</p> Source code in <code>model\\fastsam_inference.py</code> <pre><code>def get_mask_and_contour(image_path):\n    \"\"\"\n    Convenience function to get mask and contour for measurement.\n\n    Args:\n        image_path: Path to the image file\n\n    Returns:\n        contour: Largest object contour (for measurement)\n        mask: Binary mask\n        inference_time: Time in milliseconds\n    \"\"\"\n    mask, contour, inference_time = segment_image(image_path)\n    return contour, mask, inference_time\n</code></pre>"},{"location":"api/model/inference-utils/","title":"inference_utils","text":"<p>Utility classes for managing AI model loading \u2014 background warmup threads for YOLO and FastSAM.</p>"},{"location":"api/model/inference-utils/#model.inference_utils","title":"<code>inference_utils</code>","text":""},{"location":"api/model/inference-utils/#model.inference_utils--inference-utilities","title":"Inference Utilities","text":"<p>Utilities for managing AI models, including pre-loading and warmup.</p>"},{"location":"api/model/inference-utils/#model.inference_utils-classes","title":"Classes","text":""},{"location":"api/model/inference-utils/#model.inference_utils.ModelWarmupWorker","title":"<code>ModelWarmupWorker(parent=None)</code>","text":"<p>               Bases: <code>QThread</code></p> <p>Worker thread to pre-load AI models (YOLO and FastSAM) in the background. This eliminates the lag when the user first tries to measure something.</p> Source code in <code>model\\inference_utils.py</code> <pre><code>def __init__(self, parent=None):\n    super().__init__(parent)\n    self.logger = get_detection_logger()\n</code></pre>"},{"location":"api/model/measure-live/","title":"measure_live_sandals","text":"<p>Live camera measurement module \u2014 real-time detection and measurement with support for all detection methods.</p>"},{"location":"api/model/measure-live/#model.measure_live_sandals","title":"<code>measure_live_sandals</code>","text":""},{"location":"api/model/measure-live/#model.measure_live_sandals-functions","title":"Functions","text":""},{"location":"api/model/measure-live/#model.measure_live_sandals.endpoints_via_minrect","title":"<code>endpoints_via_minrect(contour)</code>","text":"Source code in <code>model\\measure_live_sandals.py</code> <pre><code>def endpoints_via_minrect(contour):\n    rect = cv2.minAreaRect(contour)\n    box = cv2.boxPoints(rect)\n    box = np.intp(box)\n    (center), (w, h), angle = rect\n    return box, w, h, angle\n</code></pre>"},{"location":"api/model/measure-live/#model.measure_live_sandals.find_largest_contours","title":"<code>find_largest_contours(mask, num_contours=2)</code>","text":"Source code in <code>model\\measure_live_sandals.py</code> <pre><code>def find_largest_contours(mask, num_contours=2):\n    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = sorted(cnts, key=lambda c: cv2.contourArea(c), reverse=True)\n    return cnts[:num_contours]\n</code></pre>"},{"location":"api/model/measure-live/#model.measure_live_sandals.measure_live_sandals","title":"<code>measure_live_sandals(input_data, mm_per_px=None, draw_output=True, save_out=None, use_sam=False, use_yolo=False, use_advanced=False)</code>","text":"<p>Live camera measurement with improved detection.</p> <p>Parameters:</p> Name Type Description Default <code>use_yolo</code> <p>If True, use YOLOv8-seg for detection (recommended)</p> <code>False</code> <code>use_advanced</code> <p>If True, use YOLOv8-X + SAM pipeline</p> <code>False</code> <p>Returns:</p> Type Description <p>results list, processed image array (with text drawn)</p> Source code in <code>model\\measure_live_sandals.py</code> <pre><code>def measure_live_sandals(input_data, mm_per_px=None, draw_output=True, save_out=None, use_sam=False, use_yolo=False, use_advanced=False):\n    \"\"\"\n    Live camera measurement with improved detection.\n\n    Args:\n        use_yolo: If True, use YOLOv8-seg for detection (recommended)\n        use_advanced: If True, use YOLOv8-X + SAM pipeline\n\n    Returns:\n        results list, processed image array (with text drawn)\n    \"\"\"\n    # Load image\n    if isinstance(input_data, str):\n        img = cv2.imread(input_data)\n        if img is None:\n            raise FileNotFoundError(f\"Cannot read image: {input_data}\")\n    else:\n        img = input_data.copy()\n\n    inference_time = 0\n\n    if use_advanced:\n        # Use Advanced (YOLOv8-X + SAM)\n        try:\n            from .advanced_inference import segment_image as advanced_segment\n            mask, contour, inference_time = advanced_segment(img)\n            if contour is not None:\n                contours = [contour]\n            else:\n                print(\"[LIVE] Advanced failed, using auto_select_mask\")\n                mask = auto_select_mask(img)\n                contours = find_largest_contours(mask, num_contours=1)\n        except ImportError as e:\n            print(f\"[LIVE] Advanced not available: {e}, using auto_select_mask\")\n            mask = auto_select_mask(img)\n            contours = find_largest_contours(mask, num_contours=1)\n    elif use_yolo:\n        # Use YOLO-Seg (recommended for all objects)\n        try:\n            from .yolo_inference import segment_image\n            mask, contour, inference_time = segment_image(img)\n            if contour is not None:\n                contours = [contour]\n            else:\n                print(\"[LIVE] YOLO failed, using auto_select_mask\")\n                mask = auto_select_mask(img)\n                contours = find_largest_contours(mask, num_contours=1)\n        except ImportError as e:\n            print(f\"[LIVE] YOLO not available: {e}, using auto_select_mask\")\n            mask = auto_select_mask(img)\n            contours = find_largest_contours(mask, num_contours=1)\n    elif use_sam:\n        # Use FastSAM\n        try:\n            from .fastsam_inference import segment_image\n            mask, contour, inference_time = segment_image(img)\n            if contour is not None:\n                contours = [contour]\n            else:\n                print(\"[LIVE] SAM failed, using auto_select_mask\")\n                mask = auto_select_mask(img)\n                contours = find_largest_contours(mask, num_contours=1)\n        except ImportError:\n            mask = auto_select_mask(img)\n            contours = find_largest_contours(mask, num_contours=1)\n    else:\n        # Use improved auto_select_mask (with strong_mask for beige detection)\n        mask = auto_select_mask(img)\n        contours = find_largest_contours(mask, num_contours=1)\n\n    out = img.copy()\n    results = []\n\n    for cnt in contours:\n        if cv2.contourArea(cnt) &lt; 1000:\n            continue\n\n        box, w, h, angle = endpoints_via_minrect(cnt)\n\n        # Use refined_endpoints for better length measurement\n        px_length = refined_endpoints(cnt)\n        px_width = min(w, h)\n\n        # Conversions\n        real_length_mm = px_length * mm_per_px if mm_per_px else None\n        real_width_mm  = px_width  * mm_per_px if mm_per_px else None\n\n        real_length_cm = real_length_mm / 10.0 if real_length_mm else None\n        real_width_cm  = real_width_mm / 10.0 if real_width_mm else None\n\n        # PASS/FAIL (dummy logic)\n        if real_length_cm:\n            pass_fail = \"PASS\" if real_length_cm &gt; 20 else \"FAIL\"\n        else:\n            pass_fail = \"UNKNOWN\"\n\n        # Draw contour + box with method-specific colors\n        if use_advanced:\n            contour_color = (0, 255, 128)  # Green-Cyan for Advanced\n        elif use_yolo:\n            contour_color = (0, 165, 255)  # Orange for YOLO\n        elif use_sam:\n            contour_color = (255, 0, 255)  # Magenta for SAM\n        else:\n            contour_color = (255, 255, 0)  # Cyan for standard\n        cv2.drawContours(out, [cnt], -1, contour_color, 2)\n        cv2.drawContours(out, [box], 0, (0, 255, 0), 2)\n\n        # Put text at TOP-LEFT of image (not near the box)\n        if draw_output:\n            # Fixed position at top-left of frame\n            x, y = 20, 35\n\n            # Draw background for readability (bigger for more info)\n            cv2.rectangle(out, (10, 10), (320, 155), (0, 0, 0), -1)\n\n            method_text = \"Method: \"\n            if use_advanced:\n                method_text += \"Advanced (YOLO-X + SAM)\"\n            elif use_yolo:\n                method_text += \"YOLO-Seg (AI)\"\n            elif use_sam:\n                method_text += \"FastSAM (AI)\"\n            else:\n                method_text += \"Standard (Beige Ready)\"\n            cv2.putText(out, method_text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n            cv2.putText(out, f\"Length: {real_length_mm:.1f} mm ({px_length:.0f} px)\",\n                        (x, y + 25), cv2.FONT_HERSHEY_SIMPLEX,\n                        0.55, (0, 255, 255), 2)\n\n            cv2.putText(out, f\"Width:  {real_width_mm:.1f} mm ({px_width:.0f} px)\",\n                        (x, y + 50), cv2.FONT_HERSHEY_SIMPLEX,\n                        0.55, (0, 200, 255), 2)\n\n            cv2.putText(out, f\"Scale: {mm_per_px:.6f} mm/px\",\n                        (x, y + 75), cv2.FONT_HERSHEY_SIMPLEX,\n                        0.5, (200, 200, 200), 1)\n\n            inf_text = f\"L: {real_length_cm:.2f}cm W: {real_width_cm:.2f}cm\"\n            if (use_sam or use_advanced) and inference_time &gt; 0:\n                inf_text += f\" ({inference_time:.0f}ms)\"\n\n            cv2.putText(out, inf_text,\n                        (x, y + 100), cv2.FONT_HERSHEY_SIMPLEX,\n                        0.5, (150, 255, 150), 1)\n\n        # Add to results dict\n        results.append({\n            \"px_length\": float(px_length),\n            \"px_width\": float(px_width),\n            \"real_length_mm\": float(real_length_mm) if real_length_mm else None,\n            \"real_width_mm\": float(real_width_mm) if real_width_mm else None,\n            \"real_length_cm\": float(real_length_cm) if real_length_cm else None,\n            \"real_width_cm\": float(real_width_cm) if real_width_cm else None,\n            \"pass_fail\": pass_fail,\n            \"inference_time_ms\": inference_time if (use_sam or use_advanced or use_yolo) else 0\n        })\n\n    if save_out:\n        ensure_dir(os.path.dirname(save_out))\n        cv2.imwrite(save_out, out)\n\n    return results, out\n</code></pre>"},{"location":"api/model/measurement/","title":"measurement","text":"<p>Core measurement logic for the QC Detector system \u2014 mask generation, contour scoring, endpoint detection, and the main measurement pipeline.</p>"},{"location":"api/model/measurement/#model.measurement","title":"<code>measurement</code>","text":""},{"location":"api/model/measurement/#model.measurement-functions","title":"Functions","text":""},{"location":"api/model/measurement/#model.measurement.evaluate_mask_quality","title":"<code>evaluate_mask_quality(mask)</code>","text":"<p>Scoring system to judge if a mask is 'Good' (Smooth &amp; Solid) or 'Bad' (Jagged &amp; Hollow). Returns a score between 0.0 and 1.0.</p> Source code in <code>model\\measurement.py</code> <pre><code>def evaluate_mask_quality(mask):\n    \"\"\"\n    Scoring system to judge if a mask is 'Good' (Smooth &amp; Solid) or 'Bad' (Jagged &amp; Hollow).\n    Returns a score between 0.0 and 1.0.\n    \"\"\"\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if not contours:\n        return 0.0\n\n    # Get largest contour\n    c = max(contours, key=cv2.contourArea)\n    area = cv2.contourArea(c)\n    perimeter = cv2.arcLength(c, True)\n\n    if area &lt; 1000 or perimeter == 0:\n        return 0.0\n\n    # 1. Solidity (Area / Convex Hull Area) - Checks if object is \"filled\"\n    hull = cv2.convexHull(c)\n    hull_area = cv2.contourArea(hull)\n    solidity = area / (hull_area + 1e-6)\n\n    # 2. Compactness (4*pi*Area / Perimeter^2) - Checks if edges are smooth\n    # A circle is 1.0. A jagged noisy shape is close to 0.\n    compactness = (4 * np.pi * area) / (perimeter ** 2)\n\n    # Weighted Score: Solidity is most important to avoid \"Hollow\" green noise\n    score = (solidity * 0.6) + (compactness * 0.4)\n    return score\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.get_channel_mask","title":"<code>get_channel_mask(img, channel_type)</code>","text":"<p>Generates a mask based on a specific strategy</p> Source code in <code>model\\measurement.py</code> <pre><code>def get_channel_mask(img, channel_type):\n    \"\"\"Generates a mask based on a specific strategy\"\"\"\n    h, w = img.shape[:2]\n\n    # Setup Background Sampling (Corners)\n    mask_corners = np.zeros((h, w), dtype=np.uint8)\n    corner_size = max(5, int(min(w, h) * 0.10))\n    cv2.rectangle(mask_corners, (0, 0), (corner_size, corner_size), 255, -1)\n    cv2.rectangle(mask_corners, (w-corner_size, 0), (w, corner_size), 255, -1)\n    cv2.rectangle(mask_corners, (0, h-corner_size), (corner_size, h), 255, -1)\n    cv2.rectangle(mask_corners, (w-corner_size, h-corner_size), (w, h), 255, -1)\n\n    raw_mask = None\n\n    if channel_type == 'A': # LAB 'A' Channel (Green-Red axis)\n        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n        _, A, _ = cv2.split(lab)\n        # Otsu threshold on A channel is usually perfect for Black/White/Color on Green\n        _, raw_mask = cv2.threshold(A, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n        # Check if we need to invert (if corners are white, invert)\n        corner_mean = cv2.mean(raw_mask, mask=mask_corners)[0]\n        if corner_mean &gt; 127:\n            raw_mask = cv2.bitwise_not(raw_mask)\n\n    elif channel_type == 'S': # HSV 'S' Channel (Saturation)\n        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n        _, S, _ = cv2.split(hsv)\n\n        # Calculate difference from background saturation\n        mean_bg_sat = cv2.mean(S, mask=mask_corners)[0]\n        diff_sat = cv2.absdiff(S, int(mean_bg_sat))\n\n        # Threshold: Background is hyper-saturated, Sandal is matte.\n        # Fixed threshold (25) often works better than Otsu for saturation diff\n        _, raw_mask = cv2.threshold(diff_sat, 25, 255, cv2.THRESH_BINARY)\n\n    elif channel_type == 'L': # LAB 'L' Channel (Lightness) - Fallback\n        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n        L, _, _ = cv2.split(lab)\n        # Adaptive threshold for contrast\n        raw_mask = cv2.adaptiveThreshold(L, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n                                        cv2.THRESH_BINARY_INV, 21, 5)\n\n    # Clean up the mask\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n    clean_mask = cv2.morphologyEx(raw_mask, cv2.MORPH_OPEN, kernel, iterations=1)\n    clean_mask = cv2.morphologyEx(clean_mask, cv2.MORPH_CLOSE, kernel, iterations=3)\n\n    # Floodfill to ensure it's solid (crucial for scoring)\n    h_m, w_m = clean_mask.shape\n    mask_flood = np.zeros((h_m+2, w_m+2), np.uint8)\n    flooded = clean_mask.copy()\n    cv2.floodFill(flooded, mask_flood, (0, 0), 255)\n    filled_mask = cv2.bitwise_not(flooded)\n    final_mask = clean_mask | filled_mask\n\n    return final_mask\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.strong_mask","title":"<code>strong_mask(img)</code>","text":"<p>Adaptive 'Tournament' Segmentation. Strategically picks the best method (Color vs Saturation) based on quality.</p> Source code in <code>model\\measurement.py</code> <pre><code>def strong_mask(img):\n    \"\"\"\n    Adaptive 'Tournament' Segmentation.\n    Strategically picks the best method (Color vs Saturation) based on quality.\n    \"\"\"\n\n    # 1. Try STRATEGY A (Color / A-Channel)\n    # This is the champion for Black, White, and Patterned sandals.\n    mask_a = get_channel_mask(img, 'A')\n    score_a = evaluate_mask_quality(mask_a)\n\n    # If Strategy A is very good (Solid &amp; Smooth), STOP HERE.\n    # This prevents the noisy 'Saturation' logic from ruining a good Black sandal mask.\n    if score_a &gt; 0.65:\n        # print(f\"[strong_mask] Fast Success: A-Channel (Score: {score_a:.2f})\")\n        return mask_a\n\n    # 2. Try STRATEGY B (Saturation / S-Channel)\n    # We only run this if A-Channel failed (likely a Green Sandal).\n    mask_s = get_channel_mask(img, 'S')\n    score_s = evaluate_mask_quality(mask_s)\n\n    # Compare candidates\n    if score_s &gt; score_a:\n        # print(f\"[strong_mask] Winner: S-Channel (Score: {score_s:.2f} vs A: {score_a:.2f})\")\n        return mask_s\n    else:\n        # print(f\"[strong_mask] Winner: A-Channel (Score: {score_a:.2f} vs S: {score_s:.2f})\")\n        return mask_a\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.auto_select_mask","title":"<code>auto_select_mask(img)</code>","text":"Source code in <code>model\\measurement.py</code> <pre><code>def auto_select_mask(img):\n    # Standard fallback logic\n    return strong_mask(img)\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.endpoints_via_minrect","title":"<code>endpoints_via_minrect(contour)</code>","text":"Source code in <code>model\\measurement.py</code> <pre><code>def endpoints_via_minrect(contour):\n    rect = cv2.minAreaRect(contour)\n    box = cv2.boxPoints(rect)\n    box = np.intp(box)\n    (center), (w, h), angle = rect\n    return box, w, h, angle\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.find_largest_contours","title":"<code>find_largest_contours(mask, num_contours=2)</code>","text":"Source code in <code>model\\measurement.py</code> <pre><code>def find_largest_contours(mask, num_contours=2):\n    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = sorted(cnts, key=lambda c: cv2.contourArea(cv2.convexHull(c)), reverse=True)\n    return cnts[:num_contours]\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.principal_axis","title":"<code>principal_axis(contour)</code>","text":"Source code in <code>model\\measurement.py</code> <pre><code>def principal_axis(contour):\n    pts = contour.reshape(-1, 2).astype(np.float32)\n    mean = np.mean(pts, axis=0)\n    cov = np.cov(pts.T)\n    eigenvalues, eigenvectors = np.linalg.eig(cov)\n    idx = np.argmax(eigenvalues)\n    direction = eigenvectors[:, idx]\n    direction /= np.linalg.norm(direction)\n    return mean, direction\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.project_onto_axis","title":"<code>project_onto_axis(contour, origin, direction)</code>","text":"Source code in <code>model\\measurement.py</code> <pre><code>def project_onto_axis(contour, origin, direction):\n    pts = contour.reshape(-1, 2).astype(np.float32)\n    vecs = pts - origin\n    proj = vecs @ direction\n    return proj\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.refined_endpoints","title":"<code>refined_endpoints(contour)</code>","text":"Source code in <code>model\\measurement.py</code> <pre><code>def refined_endpoints(contour):\n    origin, axis = principal_axis(contour)\n    proj = project_onto_axis(contour, origin, axis)\n    order = np.argsort(proj)\n    proj_sorted = proj[order]\n\n    n = len(proj_sorted)\n    k = max(int(0.05 * n), 10)\n\n    xL = proj_sorted[:k]\n    yL = np.arange(len(xL))\n    aL, bL = np.polyfit(yL, xL, 1)\n    left = aL * (-0.5) + bL\n\n    xR = proj_sorted[-k:]\n    yR = np.arange(len(xR))\n    aR, bR = np.polyfit(yR, xR, 1)\n    right = aR * (len(xR) - 0.5) + bR\n\n    length_px = right - left\n    return length_px\n</code></pre>"},{"location":"api/model/measurement/#model.measurement.measure_sandals","title":"<code>measure_sandals(path, mm_per_px=None, draw_output=True, save_out=None, use_sam=False, use_yolo=False, use_advanced=False)</code>","text":"Source code in <code>model\\measurement.py</code> <pre><code>def measure_sandals(path, mm_per_px=None, draw_output=True, save_out=None, use_sam=False, use_yolo=False, use_advanced=False):\n    img = cv2.imread(path)\n    if img is None:\n        raise FileNotFoundError(f\"Cannot read image: {path}\")\n\n    out = img.copy()\n    results = []\n    inference_time = 0\n    detection_method = 'standard'\n\n    if use_advanced:\n        detection_method = 'advanced (yolox+sam)'\n        try:\n            from .advanced_inference import segment_image as advanced_segment\n            mask, contour, inference_time = advanced_segment(img)\n\n            if contour is not None:\n                contours = [contour]\n            else:\n                print(\"[Advanced] No contour found, falling back to standard method\")\n                mask = auto_select_mask(img)\n                contours = find_largest_contours(mask, num_contours=1)\n                detection_method = 'standard (fallback)'\n\n        except ImportError as e:\n            print(f\"[Advanced] Not available: {e}, using standard method\")\n            mask = auto_select_mask(img)\n            contours = find_largest_contours(mask, num_contours=1)\n            detection_method = 'standard (fallback)'\n    elif use_yolo:\n        detection_method = 'yolo-seg'\n        try:\n            from .yolo_inference import segment_image\n            mask, contour, inference_time = segment_image(img)\n\n            if contour is not None:\n                contours = [contour]\n            else:\n                print(\"[YOLO-Seg] No contour found, falling back to standard method\")\n                mask = auto_select_mask(img)\n                contours = find_largest_contours(mask, num_contours=1)\n\n        except ImportError as e:\n            print(f\"[YOLO-Seg] YOLOv8-seg not available: {e}, using standard method\")\n            mask = auto_select_mask(img)\n            contours = find_largest_contours(mask, num_contours=1)\n    else:\n        mask = auto_select_mask(img)\n        contours = find_largest_contours(mask, num_contours=1)\n\n    for cnt in contours:\n        contour_area = cv2.contourArea(cnt)\n        if contour_area &lt; 2000:\n            continue\n\n        box, w, h, angle = endpoints_via_minrect(cnt)\n        px_length = refined_endpoints(cnt)\n        px_width  = min(w, h)\n\n        real_length = px_length * mm_per_px if mm_per_px else None\n        real_width  = px_width * mm_per_px if mm_per_px else None\n\n        cv2.drawContours(out, [cnt], -1, (255, 255, 0), 2)\n        cv2.drawContours(out, [box], 0, (0, 255, 0), 2)\n\n        result_dict = {\n            'px_length': float(px_length),\n            'px_width': float(px_width),\n            'real_length_mm': float(real_length) if real_length else None,\n            'real_width_mm': float(real_width) if real_width else None,\n            'contour_area_px': float(contour_area),\n            'detection_method': detection_method\n        }\n\n        if inference_time &gt; 0:\n            result_dict['inference_time_ms'] = inference_time\n\n        results.append(result_dict)\n\n    if save_out:\n        ensure_dir(os.path.dirname(save_out))\n        cv2.imwrite(save_out, out)\n\n    return results, out\n</code></pre>"},{"location":"api/model/preprocessor/","title":"preprocessor","text":"<p>Image preprocessing utilities \u2014 mask generation and directory management.</p>"},{"location":"api/model/preprocessor/#model.preprocessor","title":"<code>preprocessor</code>","text":""},{"location":"api/model/yolo-inference/","title":"yolo_inference","text":"<p>YOLOv8n-seg inference module with hybrid ROI approach and \"Safety Net\" fallback.</p> <p>Uses the nano variant (<code>yolov8n-seg.pt</code>, ~13 MB) optimized for CPU inference, with an IoU-based comparison between AI mask and precision-refined mask.</p>"},{"location":"api/model/yolo-inference/#model.yolo_inference","title":"<code>yolo_inference</code>","text":"<p>YOLOv8-seg Inference Module</p> <p>Segmentation using YOLOv8n-seg (nano variant optimized for CPU). Includes a \"Safety Net\" that falls back to the raw YOLO mask if precision refinement fails (e.g., Green Sandal on Green Screen).</p>"},{"location":"api/model/yolo-inference/#model.yolo_inference-functions","title":"Functions","text":""},{"location":"api/model/yolo-inference/#model.yolo_inference.get_model","title":"<code>get_model()</code>","text":"<p>Load YOLOv8n-seg model (lazy loading with caching). Downloads ~6.7MB model on first use.</p> Source code in <code>model\\yolo_inference.py</code> <pre><code>def get_model():\n    \"\"\"\n    Load YOLOv8n-seg model (lazy loading with caching).\n    Downloads ~6.7MB model on first use.\n    \"\"\"\n    global _model, _model_path\n\n    if _model is not None:\n        return _model\n\n    try:\n        from ultralytics import YOLO\n\n        # YOLOv8n-seg is the nano variant (~6.7MB, optimized for CPU)\n        model_name = \"yolov8n-seg.pt\"\n\n        # Store in model folder\n        model_dir = os.path.dirname(os.path.abspath(__file__))\n        _model_path = os.path.join(model_dir, model_name)\n\n        print(f\"[YOLOv8-seg] Loading model: {model_name}\")\n\n        # YOLO will auto-download if not present\n        _model = YOLO(model_name)\n\n        print(\"[YOLOv8-seg] Model loaded successfully\")\n        return _model\n\n    except ImportError:\n        print(\"[YOLOv8-seg] Error: ultralytics not installed. Run: pip install ultralytics\")\n        return None\n    except Exception as e:\n        print(f\"[YOLOv8-seg] Error loading model: {e}\")\n        return None\n</code></pre>"},{"location":"api/model/yolo-inference/#model.yolo_inference.segment_image","title":"<code>segment_image(image, conf=0.25)</code>","text":"<p>Segment the image using YOLOv8n-seg.</p> Source code in <code>model\\yolo_inference.py</code> <pre><code>def segment_image(image, conf=0.25):\n    \"\"\"\n    Segment the image using YOLOv8n-seg.\n    \"\"\"\n    import time\n\n    model = get_model()\n    if model is None:\n        return None, None, 0\n\n    # Load image if path is provided\n    if isinstance(image, str):\n        img = cv2.imread(image)\n    else:\n        img = image\n\n    if img is None:\n        print(\"[YOLOv8-seg] Error: Could not load image\")\n        return None, None, 0\n\n    h, w = img.shape[:2]\n    img_area = h * w\n    img_center = (w / 2, h / 2)\n\n    start_time = time.time()\n\n    # Run inference\n    results = model(\n        img,\n        device='cuda' if __import__('torch').cuda.is_available() else 'cpu',\n        conf=conf,\n        verbose=False\n    )\n\n    inference_time = (time.time() - start_time) * 1000  # Convert to ms\n\n    if len(results) == 0 or results[0].masks is None:\n        print(\"[YOLOv8-seg] No objects detected\")\n        return None, None, inference_time\n\n    # Get all masks and boxes from results\n    masks = results[0].masks.data.cpu().numpy()\n    boxes = results[0].boxes.xyxy.cpu().numpy()  # Bounding boxes\n\n    if len(masks) == 0 or len(boxes) == 0:\n        print(\"[YOLOv8-seg] No masks found\")\n        return None, None, inference_time\n\n    print(f\"[YOLOv8-seg] Found {len(masks)} objects, selecting best one...\")\n\n    # Find the best detection using same scoring logic\n    best_box = None\n    best_score = -1\n    best_mask = None\n\n    for i, (mask, box) in enumerate(zip(masks, boxes)):\n        mask_resized = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n        mask_area = np.sum(mask_resized &gt; 0)\n        area_ratio = mask_area / img_area\n\n        if area_ratio &gt; 0.70: continue\n        if area_ratio &lt; 0.01: continue\n\n        moments = cv2.moments(mask_resized)\n        if moments[\"m00\"] &gt; 0:\n            cx = moments[\"m10\"] / moments[\"m00\"]\n            cy = moments[\"m01\"] / moments[\"m00\"]\n            center_dist = np.sqrt((cx - img_center[0])**2 + (cy - img_center[1])**2)\n            max_dist = np.sqrt(img_center[0]**2 + img_center[1]**2)\n            center_score = 1 - (center_dist / max_dist)\n        else:\n            center_score = 0\n\n        score = (area_ratio * 0.7) + (center_score * 0.3)\n        if score &gt; best_score:\n            best_score = score\n            best_box = box\n            best_mask = mask_resized\n\n    if best_box is None:\n        print(\"[YOLOv8-seg] No suitable detection found\")\n        return None, None, inference_time\n\n    # === ROI Extraction ===\n    x1, y1, x2, y2 = map(int, best_box)\n\n    # Expand ROI (10% margin) to prevent cutting edges\n    margin = 0.10  \n    box_w = x2 - x1\n    box_h = y2 - y1\n    x1 = max(0, int(x1 - box_w * margin))\n    y1 = max(0, int(y1 - box_h * margin))\n    x2 = min(w, int(x2 + box_w * margin))\n    y2 = min(h, int(y2 + box_h * margin))\n\n    roi = img[y1:y2, x1:x2]\n    if roi.size == 0: return None, None, inference_time\n\n    # === SAFETY NET STRATEGY ===\n    # 1. Get the Raw YOLO Mask for this ROI\n    yolo_mask_roi = best_mask[y1:y2, x1:x2]\n    _, yolo_mask_roi = cv2.threshold(yolo_mask_roi, 0.5, 255, cv2.THRESH_BINARY)\n    yolo_mask_roi = yolo_mask_roi.astype(np.uint8)\n\n    # 2. Run Precision Refinement (Strong Mask)\n    from .measurement import strong_mask\n    cv_mask_roi = strong_mask(roi)\n\n    # 3. COMPARE: Calculate Intersection over Union (IoU)\n    intersection = cv2.bitwise_and(yolo_mask_roi, cv_mask_roi)\n    union = cv2.bitwise_or(yolo_mask_roi, cv_mask_roi)\n    count_inter = cv2.countNonZero(intersection)\n    count_union = cv2.countNonZero(union)\n\n    iou = count_inter / (count_union + 1e-6)\n    print(f\"[YOLOv8-seg] Mask Agreement (IoU): {iou:.3f}\")\n\n    final_roi_mask = None\n\n    # DECISION LOGIC:\n    # If IoU &gt; 0.60: The CV mask is valid (Agrees with AI, but sharper edges).\n    # If IoU &lt; 0.60: The CV mask failed (Camouflage/Noise). Fallback to YOLO.\n    if iou &gt; 0.60:\n        # print(\"[YOLOv8-seg] Precision mask valid. Using refined mask.\")\n        final_roi_mask = cv_mask_roi\n    else:\n        print(f\"[YOLOv8-seg] Precision mask diverged (IoU={iou:.2f}). Fallback to YOLO raw mask.\")\n        final_roi_mask = cv2.morphologyEx(yolo_mask_roi, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n        if cv2.countNonZero(final_roi_mask) == 0:\n            final_roi_mask = cv_mask_roi\n\n    # === Final Contour Extraction ===\n    contours, _ = cv2.findContours(final_roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    if len(contours) == 0:\n        print(\"[YOLOv8-seg] No contour found in final mask\")\n        return None, None, inference_time\n\n    largest_contour_roi = max(contours, key=cv2.contourArea)\n\n    # Transform coordinates back to global image\n    largest_contour = largest_contour_roi.copy()\n    largest_contour[:, :, 0] += x1\n    largest_contour[:, :, 1] += y1\n\n    # Create final binary mask for display\n    mask_binary = np.zeros((h, w), dtype=np.uint8)\n    cv2.drawContours(mask_binary, [largest_contour], -1, 255, thickness=-1)\n\n    return mask_binary, largest_contour, inference_time\n</code></pre>"},{"location":"api/model/yolo-inference/#model.yolo_inference.get_mask_and_contour","title":"<code>get_mask_and_contour(image_path)</code>","text":"Source code in <code>model\\yolo_inference.py</code> <pre><code>def get_mask_and_contour(image_path):\n    mask, contour, inference_time = segment_image(image_path)\n    return contour, mask, inference_time\n</code></pre>"},{"location":"user-guide/calibration/","title":"ArUco Calibration","text":"<p>The QC Detector uses ArUco markers for automatic camera calibration, converting pixel measurements to real-world millimeters.</p>"},{"location":"user-guide/calibration/#how-it-works","title":"How It Works","text":"<ol> <li>ArUco markers with known physical dimensions are placed in the camera's field of view</li> <li>The system detects the markers and calculates their pixel size</li> <li>By comparing pixel size to known physical size, it derives the mm/pixel ratio</li> <li>All subsequent measurements use this ratio for real-world conversion</li> </ol>"},{"location":"user-guide/calibration/#setup","title":"Setup","text":""},{"location":"user-guide/calibration/#1-print-aruco-markers","title":"1. Print ArUco Markers","text":"<p>Use the included calibration sheet (<code>aruco_calibration_A3_8markers.pdf</code>) or the single marker (<code>aruco_marker_id0_50mm.pdf</code>).</p> <p>Important</p> <p>Print at 100% scale (no scaling/fit-to-page) on A3 or A4 paper. The marker's physical size must match the configured size in settings.</p>"},{"location":"user-guide/calibration/#2-place-in-camera-view","title":"2. Place in Camera View","text":"<p>Position the printed marker(s) within the camera's field of view, on the same plane as the objects being measured.</p>"},{"location":"user-guide/calibration/#3-configure-settings","title":"3. Configure Settings","text":"<p>Go to Settings and set:</p> <ul> <li>Marker Size (mm): Physical size of each marker (default: <code>50.0 mm</code>)</li> <li>Dictionary: ArUco dictionary type (default: <code>DICT_4X4_250</code>)</li> </ul>"},{"location":"user-guide/calibration/#4-auto-calibrate","title":"4. Auto-Calibrate","text":"<p>The system automatically detects visible markers and calculates the mm/pixel ratio in real-time.</p>"},{"location":"user-guide/calibration/#multi-marker-calibration","title":"Multi-Marker Calibration","text":"<p>When 2+ markers are detected, the system uses inter-marker distance calculation for higher precision:</p> <ol> <li>Detects all visible marker centers</li> <li>Calculates pixel distances between marker pairs</li> <li>Compares with known physical distances from the calibration sheet layout</li> <li>Averages multiple pair estimates for robust calibration</li> </ol> <p>Precision Tip</p> <p>Using 3+ markers significantly improves calibration accuracy by averaging out individual detection errors.</p>"},{"location":"user-guide/calibration/#known-marker-positions-a3-sheet","title":"Known Marker Positions (A3 Sheet)","text":"Marker ID Position (mm) Location 0 (45.0, 252.0) Row 1 Left 1 (210.0, 252.0) Row 1 Center 2 (375.0, 252.0) Row 1 Right 3 (45.0, 148.5) Row 2 Left 4 (375.0, 148.5) Row 2 Right 5 (45.0, 45.0) Row 3 Left 6 (210.0, 45.0) Row 3 Center 7 (375.0, 45.0) Row 3 Right"},{"location":"user-guide/camera-setup/","title":"Camera Setup","text":""},{"location":"user-guide/camera-setup/#usb-camera","title":"USB Camera","text":"<ol> <li>Connect the USB camera to your computer</li> <li>Go to Settings in the application</li> <li>Select camera index from dropdown (0, 1, 2...)</li> <li>Adjust resolution if needed</li> </ol> <p>Tip</p> <p>Run <code>python find_camera.py</code> to list all available camera indices before launching the app.</p>"},{"location":"user-guide/camera-setup/#ip-camera-rtsp","title":"IP Camera (RTSP)","text":"<ol> <li>Go to Settings \u2192 IP Camera Configuration</li> <li>Add a new preset with:</li> </ol> Field Example Value Description Protocol <code>rtsp</code> Streaming protocol IP Address <code>192.168.1.100</code> Camera's network IP Port <code>554</code> RTSP port (default: 554) Path <code>/Streaming/Channels/101</code> Manufacturer-specific stream path Username <code>admin</code> Camera credentials (if required) Password <code>password</code> Camera credentials (if required)"},{"location":"user-guide/camera-setup/#common-rtsp-paths-by-manufacturer","title":"Common RTSP Paths by Manufacturer","text":"Manufacturer RTSP Path Hikvision <code>/Streaming/Channels/101</code> Dahua <code>/cam/realmonitor?channel=1&amp;subtype=0</code> Generic ONVIF <code>/onvif1</code> <p>Warning</p> <p>Ensure your PC is on the same subnet as the camera (e.g., <code>192.168.1.x</code>). Test connectivity with <code>ping &lt;camera-ip&gt;</code> before configuring.</p>"},{"location":"user-guide/detection-methods/","title":"Detection Methods","text":"<p>QC Detector supports four detection methods, each optimized for different scenarios.</p>"},{"location":"user-guide/detection-methods/#standard-contour","title":"Standard (Contour)","text":"<p>Best for: Dark objects with good contrast</p> <ul> <li>Traditional OpenCV edge detection</li> <li>HSV color filtering for white objects on green background</li> <li>Fast and CPU-efficient</li> <li>Good for controlled environments</li> </ul> <p>How it works:</p> <ol> <li>Image preprocessing (color space conversion)</li> <li>Adaptive thresholding and contour detection</li> <li>Mask quality evaluation (smoothness and solidity scoring)</li> <li>Best mask selection from multiple strategies</li> </ol>"},{"location":"user-guide/detection-methods/#fastsam-ai","title":"FastSAM (AI)","text":"<p>Best for: Unknown or diverse object types</p> <ul> <li>Universal object segmentation using Fast Segment Anything Model</li> <li>Good generalization across product types</li> <li>Uses <code>FastSAM-s.pt</code> (~24 MB model)</li> </ul> <p>How it works:</p> <ol> <li>Run FastSAM inference at optimal resolution</li> <li>Filter masks by area ratio (reject background and noise)</li> <li>Score candidates by center proximity and area coverage</li> <li>Apply morphological refinement to clean mask edges</li> </ol>"},{"location":"user-guide/detection-methods/#yolo-seg-ai","title":"YOLO-Seg (AI)","text":"<p>Recommended for most use cases</p> <ul> <li>Hybrid Approach: AI finds ROI \u2192 Traditional CV measures precisely</li> <li>Handles all colors equally well (including white-on-green)</li> <li>Metrology-grade edge detection with Sobel gradients</li> <li>Sub-pixel accuracy with PCA-based endpoints</li> <li>Uses <code>yolov8n-seg.pt</code> (~13 MB model)</li> </ul> <p>How it works:</p> <ol> <li>YOLOv8n-seg detects objects \u2192 bounding box + mask</li> <li>Extract ROI with 10% margin (prevents edge cutting)</li> <li>Run <code>strong_mask()</code> precision refinement inside ROI</li> <li>Compare refined mask with AI mask (IoU check)</li> <li>If IoU &gt; 0.60: use refined mask (sharper edges)</li> <li>If IoU &lt; 0.60: fallback to AI mask (camouflage scenario)</li> <li>PCA-based endpoint detection for final measurement</li> </ol> <p>Safety Net</p> <p>The IoU comparison acts as a safety net. When the precision contour diverges from the AI mask (e.g., green sandal on green screen), the system automatically falls back to the raw YOLO mask instead of producing garbage output.</p>"},{"location":"user-guide/detection-methods/#advanced-yolov8x-sam","title":"Advanced (YOLOv8x + SAM)","text":"<p>Requires GPU for best performance</p> <ul> <li>Two-Model Pipeline: YOLOv8 Extra-Large detects \u2192 SAM segments with pixel-perfect precision</li> <li>YOLOv8x (\"The Spotter\"): Extra-Large model sees texture differences invisible to smaller models</li> <li>SAM (\"The Surgeon\"): Segment Anything Model produces studio-quality masks from edge &amp; depth cues</li> <li>Uses <code>yolov8x.pt</code> (~130 MB) + <code>sam_b.pt</code> (~375 MB)</li> </ul> <p>How it works:</p> <ol> <li>YOLOv8x Detection \u2014 Finds objects even with low color contrast</li> <li>Best Object Selection \u2014 Scores candidates by area ratio, center proximity, and confidence</li> <li>Bounding Box Expansion \u2014 Adds 5% margin for SAM prompt</li> <li>SAM Segmentation \u2014 Produces pixel-perfect mask from edge &amp; depth cues</li> <li>Background Filtering \u2014 Rejects masks covering &gt;70% of image (likely background)</li> <li>Morphological Cleaning \u2014 Removes small noise artifacts</li> <li>Contour Extraction \u2014 Largest contour becomes the final measurement boundary</li> </ol> <p>Fallback Behavior</p> <p>If YOLOv8x finds no objects, the system falls back to a center-point prompt for SAM. If both models fail, it gracefully degrades to standard contour detection.</p>"},{"location":"user-guide/detection-methods/#performance-comparison","title":"Performance Comparison","text":"Method Speed Accuracy Model Size GPU Required Standard \u26a1\u26a1\u26a1 Fast Good None No FastSAM \u26a1\u26a1 Medium Very Good ~24 MB Optional YOLO-Seg \u26a1\u26a1 Medium Excellent \u2b50 ~13 MB Optional Advanced (YOLOv8x + SAM) \u26a1 Slower Best \u2b50\u2b50 ~505 MB Recommended"},{"location":"user-guide/plc-integration/","title":"PLC Integration (Modbus RTU)","text":"<p>The QC Detector can integrate with industrial PLCs via Modbus RTU for automated capture triggering and result reporting.</p>"},{"location":"user-guide/plc-integration/#configuration","title":"Configuration","text":"<p>Go to Settings and configure:</p> Setting Default Description PLC Port \u2014 COM port for Modbus communication (e.g., <code>COM3</code>) Trigger Register <code>12</code> Holding register monitored for capture trigger Result Register <code>13</code> Holding register where measurement results are written"},{"location":"user-guide/plc-integration/#trigger-flow","title":"Trigger Flow","text":"<pre><code>PLC writes 1 to Register 12\n        \u2193\nQC Detector detects 0\u21921 transition\n        \u2193\nCamera captures image immediately\n        \u2193\nMeasurement runs (selected detection method)\n        \u2193\nResult written to Register 13\n        \u2193\nQC Detector resets Register 12 to 0\n</code></pre>"},{"location":"user-guide/plc-integration/#result-values","title":"Result Values","text":"Value Range Meaning 1\u20134 Good \u2014 measurement within tolerance 5\u20138 Bad \u2014 measurement outside tolerance"},{"location":"user-guide/plc-integration/#troubleshooting","title":"Troubleshooting","text":"<p>Connection Issues</p> <ul> <li>Verify the correct COM port in Device Manager</li> <li>Check Modbus RTU settings (baud rate, parity, stop bits)</li> <li>Ensure the PLC is powered on and configured for Modbus slave mode</li> <li>Test with <code>python test_plc_modbus.py</code> to verify connectivity</li> </ul>"}]}